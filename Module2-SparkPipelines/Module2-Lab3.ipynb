{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "The following command adds the pyspark to sys.path at runtime. If the pyspark is not on the system path by default. It also prints the path of the spark.\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "Create a Spark Session\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/20 19:04:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Hyperparameter\") \\\n",
    "    .master('local[2]') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "Read the dataset into a dataframe.\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "customer_churn = spark.read.csv('customer_churn.csv', inferSchema=True, header=True, mode='DROPMALFORMED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "# Data Fields Explained\n",
    "KEY FIELDS: <br>\n",
    "1. customerID: Customer ID <br>\n",
    "2. gender: Whether the customer is a male or a female<br>\n",
    "3. SeniorCitizen: Whether the customer is a senior citizen or not (1, 0)<br>\n",
    "4. Partner: Whether the customer has a partner or not (Yes, No)<br>\n",
    "5. Dependents: Whether the customer has dependents or not (Yes, No)<br>\n",
    "6. tenure: Number of months the customer has stayed with the company<br>\n",
    "7.mPhoneService: Whether the customer has a phone service or not (Yes, No)<br>\n",
    "8. MultipleLines: Whether the customer has multiple lines or not (Yes, No, No phone service)<br>\n",
    "9. InternetService: Customer’s internet service provider (DSL, Fiber optic, No)<br>\n",
    "10. OnlineSecurity: Whether the customer has online security or not (Yes, No, No internet service)<br>\n",
    "11. OnlineBackup: Whether the customer has online backup or not (Yes, No, No internet service)<br>\n",
    "12. DeviceProtection: Whether the customer has device protection or not (Yes, No, No internet service)<br>\n",
    "13. TechSupport: Whether the customer has tech support or not (Yes, No, No internet service)<br>\n",
    "14. StreamingTV: Whether the customer has streaming TV or not (Yes, No, No internet service)<br>\n",
    "15. StreamingMovies: Whether the customer has streaming movies or not (Yes, No, No internet service)<br>\n",
    "16. Contract: The contract term of the customer (Month-to-month, One year, Two year)<br>\n",
    "17. PaperlessBilling: Whether the customer has paperless billing or not (Yes, No)<br>\n",
    "18. PaymentMethod: The customer’s payment method (Electronic check, Mailed check, Bank transfer (automatic), Credit card (automatic))<br>\n",
    "19. MonthlyCharges: The amount charged to the customer monthly<br>\n",
    "20. TotalCharges: The total amount charged to the customer<br>\n",
    "\n",
    "PREDICTOR FIELD:<br>\n",
    "1. Churn: Whether the customer churned or not (Yes or No)<br>\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------------\n",
      " customerID       | 7590-VHVEG       \n",
      " gender           | Female           \n",
      " SeniorCitizen    | 0                \n",
      " Partner          | Yes              \n",
      " Dependents       | No               \n",
      " tenure           | 1                \n",
      " PhoneService     | No               \n",
      " MultipleLines    | No phone service \n",
      " InternetService  | DSL              \n",
      " OnlineSecurity   | No               \n",
      " OnlineBackup     | Yes              \n",
      " DeviceProtection | No               \n",
      " TechSupport      | No               \n",
      " StreamingTV      | No               \n",
      " StreamingMovies  | No               \n",
      " Contract         | Month-to-month   \n",
      " PaperlessBilling | Yes              \n",
      " PaymentMethod    | Electronic check \n",
      " MonthlyCharges   | 29.85            \n",
      " TotalCharges     | 29.85            \n",
      " Churn            | No               \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_churn.show(1, truncate=False ,vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "Display the data type of the coulmns.\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, customerID: string, gender: string, SeniorCitizen: string, Partner: string, Dependents: string, tenure: string, PhoneService: string, MultipleLines: string, InternetService: string, OnlineSecurity: string, OnlineBackup: string, DeviceProtection: string, TechSupport: string, StreamingTV: string, StreamingMovies: string, Contract: string, PaperlessBilling: string, PaymentMethod: string, MonthlyCharges: string, TotalCharges: string, Churn: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(customer_churn.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "Dropping rows with NaN values\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows: 7043\n",
      "rows after dropna 7043\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"rows: {}\".format(customer_churn.count()))\n",
    "customer_churn = customer_churn.dropna()\n",
    "print(\"rows after dropna\",format(customer_churn.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "Import the pyspark modules required for pre-processing the data.\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 9:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer,VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier, NaiveBayes\n",
    "from pyspark.ml import Pipeline,Model\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "stringIndexer_1= StringIndexer(inputCol=\"gender\", outputCol=\"gender_IX\")\n",
    "stringIndexer_2 = StringIndexer(inputCol=\"Partner\", outputCol=\"Partner_IX\")\n",
    "stringIndexer_3 = StringIndexer(inputCol=\"Dependents\", outputCol=\"Dependents_IX\")\n",
    "stringIndexer_4 = StringIndexer(inputCol=\"PhoneService\", outputCol=\"PhoneService_IX\")\n",
    "stringIndexer_5 = StringIndexer(inputCol=\"MultipleLines\", outputCol=\"MultipleLines_IX\")\n",
    "stringIndexer_6 = StringIndexer(inputCol=\"InternetService\", outputCol=\"InternetService_IX\")\n",
    "stringIndexer_7 = StringIndexer(inputCol=\"OnlineSecurity\", outputCol=\"OnlineSecurity_IX\")\n",
    "stringIndexer_8 = StringIndexer(inputCol=\"OnlineBackup\", outputCol=\"OnlineBackup_IX\")\n",
    "stringIndexer_9 = StringIndexer(inputCol=\"DeviceProtection\", outputCol=\"DeviceProtection_IX\")\n",
    "stringIndexer_10 = StringIndexer(inputCol=\"TechSupport\", outputCol=\"TechSupport_IX\")\n",
    "stringIndexer_11= StringIndexer(inputCol=\"StreamingTV\", outputCol=\"StreamingTV_IX\")\n",
    "stringIndexer_12= StringIndexer(inputCol=\"StreamingMovies\", outputCol=\"StreamingMovies_IX\")\n",
    "stringIndexer_13= StringIndexer(inputCol=\"Contract\", outputCol=\"Contract_IX\")\n",
    "stringIndexer_14= StringIndexer(inputCol=\"PaperlessBilling\", outputCol=\"PaperlessBilling_IX\")\n",
    "stringIndexer_15= StringIndexer(inputCol=\"PaymentMethod\", outputCol=\"PaymentMethod_IX\")\n",
    "\n",
    "vectorAssembler_features = VectorAssembler(inputCols=[\"gender_IX\", \"Partner_IX\",\"Dependents_IX\",\"PhoneService_IX\",\"MultipleLines_IX\",\"InternetService_IX\",\"OnlineSecurity_IX\",\"OnlineBackup_IX\",\"DeviceProtection_IX\",\"TechSupport_IX\",\"StreamingTV_IX\",\"StreamingMovies_IX\",\"Contract_IX\",\"PaperlessBilling_IX\",\"PaymentMethod_IX\",\"MonthlyCharges\",\"tenure\"],outputCol=\"features\")\n",
    "stringIndexer_label = StringIndexer(inputCol=\"Churn\", outputCol=\"label\").fit(customer_churn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_churn = customer_churn.withColumn(\"TotalCharges\",customer_churn.TotalCharges.cast('float'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "Create the pre-processing pipeline\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[])\n",
    "\n",
    "basePipeline = [stringIndexer_1, stringIndexer_2,stringIndexer_3, stringIndexer_4,stringIndexer_5, stringIndexer_6,stringIndexer_7, stringIndexer_8,stringIndexer_9, stringIndexer_10,stringIndexer_11, stringIndexer_12,stringIndexer_13, stringIndexer_14,stringIndexer_15,stringIndexer_label, vectorAssembler_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "Define the Models for trying during the hyperparameter tuning and the associated hyperparameter values\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\",maxIter=10)\n",
    "pl_lr = basePipeline + [lr]\n",
    "pg_lr = ParamGridBuilder()\\\n",
    "          .baseOn({pipeline.stages: pl_lr})\\\n",
    "          .addGrid(lr.regParam,[0.01, .04])\\\n",
    "          .addGrid(lr.elasticNetParam,[0.1, 0.4])\\\n",
    "          .build()\n",
    "\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "pl_rf = basePipeline + [rf]\n",
    "pg_rf = ParamGridBuilder()\\\n",
    "      .baseOn({pipeline.stages: pl_rf})\\\n",
    "      .addGrid(rf.numTrees, [10, 25, 50])\\\n",
    "      .build()\n",
    "\n",
    "\n",
    "paramGrid = pg_lr + pg_rf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "Split the data into training and testing sets.\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training records: 4226\n",
      "Number of testing records : 2817\n"
     ]
    }
   ],
   "source": [
    "splitted_data = customer_churn.randomSplit([0.6, 0.4], 24)   \n",
    "train_data = splitted_data[0]\n",
    "test_data = splitted_data[1]\n",
    "\n",
    "print(\"Number of training records: \" + str(train_data.count()))\n",
    "print(\"Number of testing records : \" + str(test_data.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "Build a Crossvalidator for hyperparameter tuning\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/20 19:04:31 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "23/01/20 19:04:31 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import TrainValidationSplit\n",
    "cv = TrainValidationSplit(trainRatio=0.8)\\\n",
    "      .setEstimator(pipeline)\\\n",
    "      .setEvaluator(BinaryClassificationEvaluator())\\\n",
    "      .setEstimatorParamMaps(paramGrid)\n",
    "\n",
    "cvmodel = cv.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "Display the stages of the pipeline.\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexerModel: uid=StringIndexer_c76c89afded6, handleInvalid=error,\n",
       " StringIndexerModel: uid=StringIndexer_86e8bb584d57, handleInvalid=error,\n",
       " StringIndexerModel: uid=StringIndexer_ec9e2eac0273, handleInvalid=error,\n",
       " StringIndexerModel: uid=StringIndexer_4ff093ff7691, handleInvalid=error,\n",
       " StringIndexerModel: uid=StringIndexer_ea929d91e85b, handleInvalid=error,\n",
       " StringIndexerModel: uid=StringIndexer_6c0f15e1fe04, handleInvalid=error,\n",
       " StringIndexerModel: uid=StringIndexer_dd4a4615b519, handleInvalid=error,\n",
       " StringIndexerModel: uid=StringIndexer_64249960f66e, handleInvalid=error,\n",
       " StringIndexerModel: uid=StringIndexer_1ac287f1a30f, handleInvalid=error,\n",
       " StringIndexerModel: uid=StringIndexer_70f98214b685, handleInvalid=error,\n",
       " StringIndexerModel: uid=StringIndexer_b57d29378be0, handleInvalid=error,\n",
       " StringIndexerModel: uid=StringIndexer_4bd97218808f, handleInvalid=error,\n",
       " StringIndexerModel: uid=StringIndexer_bbaa55e5e43e, handleInvalid=error,\n",
       " StringIndexerModel: uid=StringIndexer_fdf71973aa63, handleInvalid=error,\n",
       " StringIndexerModel: uid=StringIndexer_5ecbcd616e5d, handleInvalid=error,\n",
       " StringIndexerModel: uid=StringIndexer_42e26681b834, handleInvalid=error,\n",
       " VectorAssembler_08d2dd86b510,\n",
       " RandomForestClassificationModel: uid=RandomForestClassifier_664256b60dc9, numTrees=25, numClasses=2, numFeatures=17]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvmodel.bestModel.stages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "Get the best model determined from the hyperparameter tuning\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cvmodel.bestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "Generate the predictions for the test data.\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "Determine the count of correct and incorrect predictions\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/20 19:05:12 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Correct predictions are  2232\n",
      "Number of In-Correct predictions are  585\n"
     ]
    }
   ],
   "source": [
    "correct = predictions.where(\"(label = prediction)\").count()\n",
    "incorrect = predictions.where(\"(label != prediction)\").count()\n",
    "\n",
    "print(\"Number of Correct predictions are \", correct)\n",
    "print(\"Number of In-Correct predictions are \", incorrect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "Display the actual label and the prediction columns\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       1.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  1.0|       0.0|\n",
      "|  1.0|       1.0|\n",
      "|  0.0|       0.0|\n",
      "|  1.0|       1.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  1.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select('label', 'prediction').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "Determine the values of the confusion matrix.\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positive 314\n",
      "True Negative 1918\n",
      "False Positive 152\n",
      "False Negative 433\n"
     ]
    }
   ],
   "source": [
    "tp = predictions.filter(\"label == 1.0 AND prediction == 1.0\").count()\n",
    "tn = predictions.where(\"label == 0.0 AND prediction == 0.0\").count()\n",
    "fp = predictions.where(\"label == 0.0 AND prediction == 1.0\").count()\n",
    "fn = predictions.where(\"label == 1.0 AND prediction == 0.0\").count()\n",
    "\n",
    "print(\"True Positive\", tp)\n",
    "print(\"True Negative\", tn)\n",
    "print(\"False Positive\", fp)\n",
    "print(\"False Negative\", fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "Determine the Precision and Recall Values.\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall 0.4203480589022758\n",
      "precision 0.6738197424892703\n"
     ]
    }
   ],
   "source": [
    "r = float(tp)/(tp + fn)\n",
    "print(\"recall\", r)\n",
    "\n",
    "p = float(tp) / (tp + fp)\n",
    "print(\"precision\", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "Stop the Spark Session\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "918816dcb90eb2f409d280fd921ab35213362b65b31855a310672842d91a763f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
