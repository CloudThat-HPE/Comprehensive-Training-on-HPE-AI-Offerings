{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0cc588f",
   "metadata": {
    "id": "c0cc588f"
   },
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "Sentiment analysis is also known as opinion mining. Sentiment analysis is a type of text mining that finds and extracts subjective information from source material, assisting businesses in determining the social sentiment associated with their brand, product, or service while monitoring online discussions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91df8baa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "91df8baa",
    "outputId": "58927452-94f9-4425-b851-ab941977fb6b"
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614a2e21",
   "metadata": {
    "id": "614a2e21"
   },
   "source": [
    "Starting the Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b821bf6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/20 19:34:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "#create SparkSession instance\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.config('spark.executor.memory','16g').appName('sentanaly').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f0151f",
   "metadata": {
    "id": "b4f0151f"
   },
   "source": [
    "Import Important modules required "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a971d950",
   "metadata": {
    "id": "a971d950"
   },
   "outputs": [],
   "source": [
    "#importing pyspark ml sql features\n",
    "from pyspark.ml import Pipeline \n",
    "from pyspark.ml.feature import CountVectorizer,StringIndexer, RegexTokenizer,StopWordsRemover\n",
    "from pyspark.sql.functions import col, udf,regexp_replace,isnull\n",
    "from pyspark.sql.types import StringType,IntegerType\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ae8f05",
   "metadata": {
    "id": "a0ae8f05"
   },
   "source": [
    "Now we are loading the dataset. The dataset used here contains the tweets with the sentiment value. The 0 represent negative sentiments and 1 represent positive sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c92d708",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25ec274d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "25ec274d",
    "outputId": "27789d30-a43b-4f58-8bda-5d46ce75748f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      "\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "|_c0|       _c1|                 _c2|     _c3|            _c4|                 _c5|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "|  0|1467810369|Mon Apr 06 22:19:...|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
      "|  0|1467810672|Mon Apr 06 22:19:...|NO_QUERY|  scotthamilton|is upset that he ...|\n",
      "|  0|1467810917|Mon Apr 06 22:19:...|NO_QUERY|       mattycus|@Kenichan I dived...|\n",
      "|  0|1467811184|Mon Apr 06 22:19:...|NO_QUERY|        ElleCTF|my whole body fee...|\n",
      "|  0|1467811193|Mon Apr 06 22:19:...|NO_QUERY|         Karoli|@nationwideclass ...|\n",
      "|  0|1467811372|Mon Apr 06 22:20:...|NO_QUERY|       joy_wolf|@Kwesidei not the...|\n",
      "|  0|1467811592|Mon Apr 06 22:20:...|NO_QUERY|        mybirch|         Need a hug |\n",
      "|  0|1467811594|Mon Apr 06 22:20:...|NO_QUERY|           coZZ|@LOLTrish hey  lo...|\n",
      "|  0|1467811795|Mon Apr 06 22:20:...|NO_QUERY|2Hood4Hollywood|@Tatiana_K nope t...|\n",
      "|  0|1467812025|Mon Apr 06 22:20:...|NO_QUERY|        mimismo|@twittera que me ...|\n",
      "|  0|1467812416|Mon Apr 06 22:20:...|NO_QUERY| erinx3leannexo|spring break in p...|\n",
      "|  0|1467812579|Mon Apr 06 22:20:...|NO_QUERY|   pardonlauren|I just re-pierced...|\n",
      "|  0|1467812723|Mon Apr 06 22:20:...|NO_QUERY|           TLeC|@caregiving I cou...|\n",
      "|  0|1467812771|Mon Apr 06 22:20:...|NO_QUERY|robrobbierobert|@octolinz16 It it...|\n",
      "|  0|1467812784|Mon Apr 06 22:20:...|NO_QUERY|    bayofwolves|@smarrison i woul...|\n",
      "|  0|1467812799|Mon Apr 06 22:20:...|NO_QUERY|     HairByJess|@iamjazzyfizzle I...|\n",
      "|  0|1467812964|Mon Apr 06 22:20:...|NO_QUERY| lovesongwriter|Hollis' death sce...|\n",
      "|  0|1467813137|Mon Apr 06 22:20:...|NO_QUERY|       armotley|about to file taxes |\n",
      "|  0|1467813579|Mon Apr 06 22:20:...|NO_QUERY|     starkissed|@LettyA ahh ive a...|\n",
      "|  0|1467813782|Mon Apr 06 22:20:...|NO_QUERY|      gi_gi_bee|@FakerPattyPattz ...|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.cache of DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string]>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read the csv containing twitter data\n",
    "news_data = spark.read.csv('trainingsentimentdata.csv',header= False)\n",
    "#printing the data\n",
    "news_data.printSchema()\n",
    "news_data.show()\n",
    "news_data = news_data.limit(500)\n",
    "news_data.cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a520a22",
   "metadata": {
    "id": "2a520a22"
   },
   "source": [
    "We can check the count of totalitems in the dataset for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca4fcc97",
   "metadata": {
    "id": "ca4fcc97",
    "outputId": "97d5acd1-c2d0-43a7-f432-836814b25368"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count the rows of dataset\n",
    "news_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00df7f9c",
   "metadata": {
    "id": "00df7f9c"
   },
   "source": [
    "We are selecting the titles of tweets and the corresponding category of each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bab7451",
   "metadata": {
    "id": "7bab7451",
    "outputId": "f5200575-50c3-42bc-ac1a-cc428074ba04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|                 _c5|_c0|\n",
      "+--------------------+---+\n",
      "|@switchfoot http:...|  0|\n",
      "|is upset that he ...|  0|\n",
      "|@Kenichan I dived...|  0|\n",
      "|my whole body fee...|  0|\n",
      "|@nationwideclass ...|  0|\n",
      "|@Kwesidei not the...|  0|\n",
      "|         Need a hug |  0|\n",
      "|@LOLTrish hey  lo...|  0|\n",
      "|@Tatiana_K nope t...|  0|\n",
      "|@twittera que me ...|  0|\n",
      "|spring break in p...|  0|\n",
      "|I just re-pierced...|  0|\n",
      "|@caregiving I cou...|  0|\n",
      "|@octolinz16 It it...|  0|\n",
      "|@smarrison i woul...|  0|\n",
      "|@iamjazzyfizzle I...|  0|\n",
      "|Hollis' death sce...|  0|\n",
      "|about to file taxes |  0|\n",
      "|@LettyA ahh ive a...|  0|\n",
      "|@FakerPattyPattz ...|  0|\n",
      "+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#pick only _c5 and _c0 columns and place in title_category\n",
    "title_category = news_data.select(\"_c5\",\"_c0\")\n",
    "title_category.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd72e0d",
   "metadata": {
    "id": "cfd72e0d"
   },
   "source": [
    "This is the custom function definition to count the null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "748d0cef",
   "metadata": {
    "id": "748d0cef"
   },
   "outputs": [],
   "source": [
    "#function to count null values in the columns\n",
    "def null_value_count(df):\n",
    "  null_columns_counts = [] #initialize array to null\n",
    "  numRows = df.count() #count the number of rows\n",
    "  for k in df.columns:\n",
    "    nullRows = df.where(col(k).isNull()).count() #count null rows\n",
    "    if(nullRows > 0):\n",
    "      temp = k,nullRows\n",
    "      null_columns_counts.append(temp)\n",
    "  return(null_columns_counts) #return count of null collumns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeaa0a96",
   "metadata": {
    "id": "aeaa0a96"
   },
   "source": [
    "We are applying the custom function to the data frsme title_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11b9d67a",
   "metadata": {
    "id": "11b9d67a"
   },
   "outputs": [],
   "source": [
    "null_columns_count_list = null_value_count(title_category)\n",
    "#spark.createDataFrame(null_columns_count_list, ['Column_With_Null_Value', 'Null_Values_Count']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1840e9",
   "metadata": {
    "id": "3b1840e9"
   },
   "source": [
    "# Cleaning the dataset\n",
    "\n",
    "Now we can drop the null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "534b2a42",
   "metadata": {
    "id": "534b2a42",
    "outputId": "e2b9fef4-4103-4680-dc9a-43d658740cb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------+---+\n",
      "|_c5                                                                                                                  |_c0|\n",
      "+---------------------------------------------------------------------------------------------------------------------+---+\n",
      "|@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  |0  |\n",
      "|is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!      |0  |\n",
      "|@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                            |0  |\n",
      "|my whole body feels itchy and like its on fire                                                                       |0  |\n",
      "|@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.       |0  |\n",
      "|@Kwesidei not the whole crew                                                                                         |0  |\n",
      "|Need a hug                                                                                                           |0  |\n",
      "|@LOLTrish hey  long time no see! Yes.. Rains a bit ,only a bit  LOL , I'm fine thanks , how's you ?                  |0  |\n",
      "|@Tatiana_K nope they didn't have it                                                                                  |0  |\n",
      "|@twittera que me muera ?                                                                                             |0  |\n",
      "|spring break in plain city... it's snowing                                                                           |0  |\n",
      "|I just re-pierced my ears                                                                                            |0  |\n",
      "|@caregiving I couldn't bear to watch it.  And I thought the UA loss was embarrassing . . . . .                       |0  |\n",
      "|@octolinz16 It it counts, idk why I did either. you never talk to me anymore                                         |0  |\n",
      "|@smarrison i would've been the first, but i didn't have a gun.    not really though, zac snyder's just a doucheclown.|0  |\n",
      "|@iamjazzyfizzle I wish I got to watch it with you!! I miss you and @iamlilnicki  how was the premiere?!              |0  |\n",
      "|Hollis' death scene will hurt me severely to watch on film  wry is directors cut not out now?                        |0  |\n",
      "|about to file taxes                                                                                                  |0  |\n",
      "|@LettyA ahh ive always wanted to see rent  love the soundtrack!!                                                     |0  |\n",
      "|@FakerPattyPattz Oh dear. Were you drinking out of the forgotten table drinks?                                       |0  |\n",
      "+---------------------------------------------------------------------------------------------------------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#drop not applicable and null values from the category\n",
    "title_category = title_category.dropna()\n",
    "title_category.count()\n",
    "title_category.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c86548d4",
   "metadata": {
    "id": "c86548d4",
    "outputId": "2e945b46-fe60-443c-a180-e647f58ca78d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Tweets: string (nullable = true)\n",
      " |-- Sentiment: string (nullable = true)\n",
      "\n",
      "+--------------------+---------+\n",
      "|              Tweets|Sentiment|\n",
      "+--------------------+---------+\n",
      "|@switchfoot http:...|        0|\n",
      "|is upset that he ...|        0|\n",
      "|@Kenichan I dived...|        0|\n",
      "|my whole body fee...|        0|\n",
      "|@nationwideclass ...|        0|\n",
      "|@Kwesidei not the...|        0|\n",
      "|         Need a hug |        0|\n",
      "|@LOLTrish hey  lo...|        0|\n",
      "|@Tatiana_K nope t...|        0|\n",
      "|@twittera que me ...|        0|\n",
      "|spring break in p...|        0|\n",
      "|I just re-pierced...|        0|\n",
      "|@caregiving I cou...|        0|\n",
      "|@octolinz16 It it...|        0|\n",
      "|@smarrison i woul...|        0|\n",
      "|@iamjazzyfizzle I...|        0|\n",
      "|Hollis' death sce...|        0|\n",
      "|about to file taxes |        0|\n",
      "|@LettyA ahh ive a...|        0|\n",
      "|@FakerPattyPattz ...|        0|\n",
      "+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "#data containing from the csv\n",
    "oldColumns = title_category.schema.names\n",
    "#creating new columns with heading 'Tweets' and 'Sentiment'\n",
    "newColumns = ['Tweets','Sentiment']\n",
    "\n",
    "#applying lambda function to create a copy of old columns to new columns\n",
    "title_category = reduce(lambda title_category, idx: title_category.withColumnRenamed(oldColumns[idx], newColumns[idx]),range(len(oldColumns)), title_category)\n",
    "title_category.printSchema()\n",
    "title_category.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad4ef83",
   "metadata": {
    "id": "8ad4ef83"
   },
   "source": [
    "Now we can remove the numbers in tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e67a8cd",
   "metadata": {
    "id": "1e67a8cd",
    "outputId": "2988a0ce-a75e-44ea-e41b-868f04362dbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------+\n",
      "|Tweets                                                                                                               |only_str                                                                                                             |\n",
      "+---------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------+\n",
      "|@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  |@switchfoot http://twitpic.com/yzl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D    |\n",
      "|is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!      |is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!      |\n",
      "|@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                            |@Kenichan I dived many times for the ball. Managed to save %  The rest go out of bounds                              |\n",
      "|my whole body feels itchy and like its on fire                                                                       |my whole body feels itchy and like its on fire                                                                       |\n",
      "|@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.       |@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.       |\n",
      "|@Kwesidei not the whole crew                                                                                         |@Kwesidei not the whole crew                                                                                         |\n",
      "|Need a hug                                                                                                           |Need a hug                                                                                                           |\n",
      "|@LOLTrish hey  long time no see! Yes.. Rains a bit ,only a bit  LOL , I'm fine thanks , how's you ?                  |@LOLTrish hey  long time no see! Yes.. Rains a bit ,only a bit  LOL , I'm fine thanks , how's you ?                  |\n",
      "|@Tatiana_K nope they didn't have it                                                                                  |@Tatiana_K nope they didn't have it                                                                                  |\n",
      "|@twittera que me muera ?                                                                                             |@twittera que me muera ?                                                                                             |\n",
      "|spring break in plain city... it's snowing                                                                           |spring break in plain city... it's snowing                                                                           |\n",
      "|I just re-pierced my ears                                                                                            |I just re-pierced my ears                                                                                            |\n",
      "|@caregiving I couldn't bear to watch it.  And I thought the UA loss was embarrassing . . . . .                       |@caregiving I couldn't bear to watch it.  And I thought the UA loss was embarrassing . . . . .                       |\n",
      "|@octolinz16 It it counts, idk why I did either. you never talk to me anymore                                         |@octolinz It it counts, idk why I did either. you never talk to me anymore                                           |\n",
      "|@smarrison i would've been the first, but i didn't have a gun.    not really though, zac snyder's just a doucheclown.|@smarrison i would've been the first, but i didn't have a gun.    not really though, zac snyder's just a doucheclown.|\n",
      "|@iamjazzyfizzle I wish I got to watch it with you!! I miss you and @iamlilnicki  how was the premiere?!              |@iamjazzyfizzle I wish I got to watch it with you!! I miss you and @iamlilnicki  how was the premiere?!              |\n",
      "|Hollis' death scene will hurt me severely to watch on film  wry is directors cut not out now?                        |Hollis' death scene will hurt me severely to watch on film  wry is directors cut not out now?                        |\n",
      "|about to file taxes                                                                                                  |about to file taxes                                                                                                  |\n",
      "|@LettyA ahh ive always wanted to see rent  love the soundtrack!!                                                     |@LettyA ahh ive always wanted to see rent  love the soundtrack!!                                                     |\n",
      "|@FakerPattyPattz Oh dear. Were you drinking out of the forgotten table drinks?                                       |@FakerPattyPattz Oh dear. Were you drinking out of the forgotten table drinks?                                       |\n",
      "+---------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#cleaning the numbers from tweets\n",
    "title_category = title_category.withColumn(\"only_str\",regexp_replace(col('Tweets'), '\\d+', ''))\n",
    "title_category.select(\"Tweets\",\"only_str\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49f034f",
   "metadata": {
    "id": "b49f034f"
   },
   "source": [
    "Split the text into constituent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1dae25af",
   "metadata": {
    "id": "1dae25af",
    "outputId": "53236734-9acb-46f6-c0aa-6ece40608478"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------------------+--------------------+\n",
      "|              Tweets|Sentiment|            only_str|               words|\n",
      "+--------------------+---------+--------------------+--------------------+\n",
      "|@switchfoot http:...|        0|@switchfoot http:...|[switchfoot, http...|\n",
      "|is upset that he ...|        0|is upset that he ...|[is, upset, that,...|\n",
      "|@Kenichan I dived...|        0|@Kenichan I dived...|[kenichan, i, div...|\n",
      "|my whole body fee...|        0|my whole body fee...|[my, whole, body,...|\n",
      "|@nationwideclass ...|        0|@nationwideclass ...|[nationwideclass,...|\n",
      "|@Kwesidei not the...|        0|@Kwesidei not the...|[kwesidei, not, t...|\n",
      "|         Need a hug |        0|         Need a hug |      [need, a, hug]|\n",
      "|@LOLTrish hey  lo...|        0|@LOLTrish hey  lo...|[loltrish, hey, l...|\n",
      "|@Tatiana_K nope t...|        0|@Tatiana_K nope t...|[tatiana_k, nope,...|\n",
      "|@twittera que me ...|        0|@twittera que me ...|[twittera, que, m...|\n",
      "|spring break in p...|        0|spring break in p...|[spring, break, i...|\n",
      "|I just re-pierced...|        0|I just re-pierced...|[i, just, re, pie...|\n",
      "|@caregiving I cou...|        0|@caregiving I cou...|[caregiving, i, c...|\n",
      "|@octolinz16 It it...|        0|@octolinz It it c...|[octolinz, it, it...|\n",
      "|@smarrison i woul...|        0|@smarrison i woul...|[smarrison, i, wo...|\n",
      "|@iamjazzyfizzle I...|        0|@iamjazzyfizzle I...|[iamjazzyfizzle, ...|\n",
      "|Hollis' death sce...|        0|Hollis' death sce...|[hollis, death, s...|\n",
      "|about to file taxes |        0|about to file taxes |[about, to, file,...|\n",
      "|@LettyA ahh ive a...|        0|@LettyA ahh ive a...|[lettya, ahh, ive...|\n",
      "|@FakerPattyPattz ...|        0|@FakerPattyPattz ...|[fakerpattypattz,...|\n",
      "+--------------------+---------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#split the text to words or tokens\n",
    "#https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.ml.feature.RegexTokenizer.html\n",
    "regex_tokenizer = RegexTokenizer(inputCol=\"only_str\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "raw_words = regex_tokenizer.transform(title_category)\n",
    "raw_words.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1526619",
   "metadata": {
    "id": "d1526619"
   },
   "source": [
    "Remove the stop words from segregated list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94d5434a",
   "metadata": {
    "id": "94d5434a",
    "outputId": "fcd3c94d-5199-4091-e677-d8980b7d2431"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------+\n",
      "|words                                                                                                                                |filtered                                                                                     |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------+\n",
      "|[switchfoot, http, twitpic, com, yzl, awww, that, s, a, bummer, you, shoulda, got, david, carr, of, third, day, to, do, it, d]       |[switchfoot, http, twitpic, com, yzl, awww, bummer, shoulda, got, david, carr, third, day, d]|\n",
      "|[is, upset, that, he, can, t, update, his, facebook, by, texting, it, and, might, cry, as, a, result, school, today, also, blah]     |[upset, update, facebook, texting, might, cry, result, school, today, also, blah]            |\n",
      "|[kenichan, i, dived, many, times, for, the, ball, managed, to, save, the, rest, go, out, of, bounds]                                 |[kenichan, dived, many, times, ball, managed, save, rest, go, bounds]                        |\n",
      "|[my, whole, body, feels, itchy, and, like, its, on, fire]                                                                            |[whole, body, feels, itchy, like, fire]                                                      |\n",
      "|[nationwideclass, no, it, s, not, behaving, at, all, i, m, mad, why, am, i, here, because, i, can, t, see, you, all, over, there]    |[nationwideclass, behaving, m, mad, see]                                                     |\n",
      "|[kwesidei, not, the, whole, crew]                                                                                                    |[kwesidei, whole, crew]                                                                      |\n",
      "|[need, a, hug]                                                                                                                       |[need, hug]                                                                                  |\n",
      "|[loltrish, hey, long, time, no, see, yes, rains, a, bit, only, a, bit, lol, i, m, fine, thanks, how, s, you]                         |[loltrish, hey, long, time, see, yes, rains, bit, bit, lol, m, fine, thanks]                 |\n",
      "|[tatiana_k, nope, they, didn, t, have, it]                                                                                           |[tatiana_k, nope, didn]                                                                      |\n",
      "|[twittera, que, me, muera]                                                                                                           |[twittera, que, muera]                                                                       |\n",
      "|[spring, break, in, plain, city, it, s, snowing]                                                                                     |[spring, break, plain, city, snowing]                                                        |\n",
      "|[i, just, re, pierced, my, ears]                                                                                                     |[re, pierced, ears]                                                                          |\n",
      "|[caregiving, i, couldn, t, bear, to, watch, it, and, i, thought, the, ua, loss, was, embarrassing]                                   |[caregiving, couldn, bear, watch, thought, ua, loss, embarrassing]                           |\n",
      "|[octolinz, it, it, counts, idk, why, i, did, either, you, never, talk, to, me, anymore]                                              |[octolinz, counts, idk, either, never, talk, anymore]                                        |\n",
      "|[smarrison, i, would, ve, been, the, first, but, i, didn, t, have, a, gun, not, really, though, zac, snyder, s, just, a, doucheclown]|[smarrison, ve, first, didn, gun, really, though, zac, snyder, doucheclown]                  |\n",
      "|[iamjazzyfizzle, i, wish, i, got, to, watch, it, with, you, i, miss, you, and, iamlilnicki, how, was, the, premiere]                 |[iamjazzyfizzle, wish, got, watch, miss, iamlilnicki, premiere]                              |\n",
      "|[hollis, death, scene, will, hurt, me, severely, to, watch, on, film, wry, is, directors, cut, not, out, now]                        |[hollis, death, scene, hurt, severely, watch, film, wry, directors, cut]                     |\n",
      "|[about, to, file, taxes]                                                                                                             |[file, taxes]                                                                                |\n",
      "|[lettya, ahh, ive, always, wanted, to, see, rent, love, the, soundtrack]                                                             |[lettya, ahh, ive, always, wanted, see, rent, love, soundtrack]                              |\n",
      "|[fakerpattypattz, oh, dear, were, you, drinking, out, of, the, forgotten, table, drinks]                                             |[fakerpattypattz, oh, dear, drinking, forgotten, table, drinks]                              |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Removing the stop words from the list of words\n",
    "#https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.StopWordsRemover.html\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "words_df = remover.transform(raw_words)\n",
    "words_df.select(\"words\",\"filtered\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e215cd",
   "metadata": {
    "id": "10e215cd"
   },
   "source": [
    "Convert text into vectors of token counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff14dc4",
   "metadata": {
    "id": "7ff14dc4"
   },
   "source": [
    "# Partition the dataset into training and test datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1efd33e",
   "metadata": {
    "id": "a1efd33e",
    "outputId": "5c763c58-2bd4-465c-8721-b09bd961d5f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------------------+--------------------+--------------------+\n",
      "|              Tweets|Sentiment|            only_str|               words|            filtered|\n",
      "+--------------------+---------+--------------------+--------------------+--------------------+\n",
      "| Body Of Missing ...|        0| Body Of Missing ...|[body, of, missin...|[body, missing, n...|\n",
      "| wonder if Jon lo...|        0| wonder if Jon lo...|[wonder, if, jon,...|[wonder, jon, los...|\n",
      "|#3 woke up and wa...|        0|# woke up and was...|[woke, up, and, w...|[woke, accident, ...|\n",
      "|&quot;On popular ...|        0|&quot;On popular ...|[quot, on, popula...|[quot, popular, m...|\n",
      "|...and, India mis...|        0|...and, India mis...|[and, india, miss...|[india, missed, t...|\n",
      "|@AmaNorris wow th...|        0|@AmaNorris wow th...|[amanorris, wow, ...|[amanorris, wow, ...|\n",
      "|@Appomattox_News ...|        0|@Appomattox_News ...|[appomattox_news,...|[appomattox_news,...|\n",
      "|@B_Barnett I did ...|        0|@B_Barnett I did ...|[b_barnett, i, di...|[b_barnett, reall...|\n",
      "|@BatManYNG I miss...|        0|@BatManYNG I miss...|[batmanyng, i, mi...|[batmanyng, miss,...|\n",
      "|@Brandizzzle08 yo...|        0|@Brandizzzle yoyo...|[brandizzzle, yoy...|[brandizzzle, yoy...|\n",
      "|@CarVin1 lol they...|        0|@CarVin lol they ...|[carvin, lol, the...|[carvin, lol, emo...|\n",
      "|@David_Henrie *th...|        0|@David_Henrie *th...|[david_henrie, th...|[david_henrie, th...|\n",
      "|@DiannePulham OOO...|        0|@DiannePulham OOO...|[diannepulham, oo...|[diannepulham, oo...|\n",
      "|@DjAlizay I reall...|        0|@DjAlizay I reall...|[djalizay, i, rea...|[djalizay, really...|\n",
      "|@DonnieWahlberg o...|        0|@DonnieWahlberg o...|[donniewahlberg, ...|[donniewahlberg, ...|\n",
      "|@EazyDoesIt87 NEG...|        0|@EazyDoesIt NEGAT...|[eazydoesit, nega...|[eazydoesit, nega...|\n",
      "|@FakerPattyPattz ...|        0|@FakerPattyPattz ...|[fakerpattypattz,...|[fakerpattypattz,...|\n",
      "|@FranzGlaus I kno...|        0|@FranzGlaus I kno...|[franzglaus, i, k...|[franzglaus, know...|\n",
      "|@GuruMN but this ...|        0|@GuruMN but this ...|[gurumn, but, thi...|[gurumn, canada, ...|\n",
      "|@Henkuyinepu yeah...|        0|@Henkuyinepu yeah...|[henkuyinepu, yea...|[henkuyinepu, yea...|\n",
      "+--------------------+---------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+---------+--------------------+--------------------+--------------------+\n",
      "|              Tweets|Sentiment|            only_str|               words|            filtered|\n",
      "+--------------------+---------+--------------------+--------------------+--------------------+\n",
      "|..and of course.....|        0|..and of course.....|[and, of, course,...|[course, access, ...|\n",
      "|@Alliana07 it did...|        0|@Alliana it didn'...|[alliana, it, did...|[alliana, didn, m...|\n",
      "|@BridgetsBeaches ...|        0|@BridgetsBeaches ...|[bridgetsbeaches,...|[bridgetsbeaches,...|\n",
      "|@Brodhe geez ur n...|        0|@Brodhe geez ur n...|[brodhe, geez, ur...|[brodhe, geez, ur...|\n",
      "|@CaitlinOConnor i...|        0|@CaitlinOConnor i...|[caitlinoconnor, ...|[caitlinoconnor, ...|\n",
      "|@ColinDeMar Far t...|        0|@ColinDeMar Far t...|[colindemar, far,...|[colindemar, far,...|\n",
      "|@Dangerm0use I th...|        0|@Dangermuse I thi...|[dangermuse, i, t...|[dangermuse, thin...|\n",
      "|@DonnieWahlberg I...|        0|@DonnieWahlberg I...|[donniewahlberg, ...|[donniewahlberg, ...|\n",
      "|@Henkuyinepu it's...|        0|@Henkuyinepu it's...|[henkuyinepu, it,...|[henkuyinepu, ove...|\n",
      "|@HibaNick yeah aw...|        0|@HibaNick yeah aw...|[hibanick, yeah, ...|[hibanick, yeah, ...|\n",
      "|@HumpNinja I cry ...|        0|@HumpNinja I cry ...|[humpninja, i, cr...|[humpninja, cry, ...|\n",
      "|@JonathanRKnight ...|        0|@JonathanRKnight ...|[jonathanrknight,...|[jonathanrknight,...|\n",
      "|@Kenichan I dived...|        0|@Kenichan I dived...|[kenichan, i, div...|[kenichan, dived,...|\n",
      "|@LevenRambin: Tak...|        0|@LevenRambin: Tak...|[levenrambin, tak...|[levenrambin, tak...|\n",
      "|@MissXu sorry! be...|        0|@MissXu sorry! be...|[missxu, sorry, b...|[missxu, sorry, b...|\n",
      "|@Starrbby too bad...|        0|@Starrbby too bad...|[starrbby, too, b...|[starrbby, bad, w...|\n",
      "|@ThaStevieG but w...|        0|@ThaStevieG but w...|[thastevieg, but,...|[thastevieg, real...|\n",
      "|@aaronrva is in t...|        0|@aaronrva is in t...|[aaronrva, is, in...|[aaronrva, bathro...|\n",
      "|@alicayaba so cuu...|        0|@alicayaba so cuu...|[alicayaba, so, c...|[alicayaba, cuuut...|\n",
      "|@austinhill I wis...|        0|@austinhill I wis...|[austinhill, i, w...|[austinhill, wish...|\n",
      "+--------------------+---------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Partition the dataset into trainingData 80% and testData 20%\n",
    "(trainingData, testData) = words_df.randomSplit([0.8, 0.2],seed = 11)\n",
    "trainingData.show()\n",
    "testData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df21afb",
   "metadata": {
    "id": "1df21afb"
   },
   "source": [
    "# Model Training and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0267532",
   "metadata": {
    "id": "e0267532",
    "outputId": "82a52fc3-bfe3-4996-c5af-acd8922e2d42"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|              Tweets|Sentiment|            only_str|               words|            filtered|                  tf|            features|\n",
      "+--------------------+---------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| Body Of Missing ...|        0| Body Of Missing ...|[body, of, missin...|[body, missing, n...|(65536,[731,3159,...|(65536,[731,3159,...|\n",
      "| wonder if Jon lo...|        0| wonder if Jon lo...|[wonder, if, jon,...|[wonder, jon, los...|(65536,[19153,329...|(65536,[19153,329...|\n",
      "|#3 woke up and wa...|        0|# woke up and was...|[woke, up, and, w...|[woke, accident, ...|(65536,[5660,7427...|(65536,[5660,7427...|\n",
      "|&quot;On popular ...|        0|&quot;On popular ...|[quot, on, popula...|[quot, popular, m...|(65536,[178,1903,...|(65536,[178,1903,...|\n",
      "|...and, India mis...|        0|...and, India mis...|[and, india, miss...|[india, missed, t...|(65536,[12350,228...|(65536,[12350,228...|\n",
      "+--------------------+---------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/20 19:34:53 WARN DAGScheduler: Broadcasting large task binary with size 1087.4 KiB\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "#tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "#HashingTF maps a sequence of terms\n",
    "#https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.HashingTF.html\n",
    "hashtf = HashingTF(numFeatures=2**16, inputCol=\"filtered\", outputCol='tf')\n",
    "\n",
    "#IDF stands for Inverse Document Frequency (Common:Rare)\n",
    "#https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.mllib.feature.IDF.html\n",
    "idf = IDF(inputCol='tf', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "#label_stringIdx = StringIndexer(inputCol = \"target\", outputCol = \"label\")\n",
    "pipeline = Pipeline(stages=[hashtf, idf])\n",
    "\n",
    "pipelineFit = pipeline.fit(trainingData)\n",
    "train_df = pipelineFit.transform(trainingData)\n",
    "val_df = pipelineFit.transform(testData)\n",
    "train_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae08819f",
   "metadata": {
    "id": "ae08819f"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "train_df=train_df.withColumnRenamed('Sentiment', 'label')\n",
    "train_df=train_df.withColumn(\"label\",train_df.label.cast('int'))\n",
    "val_df=val_df.withColumnRenamed('Sentiment', 'label')\n",
    "val_df=val_df.withColumn(\"label\",val_df.label.cast('int'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6a7f6b",
   "metadata": {
    "id": "3e6a7f6b"
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80efef2b",
   "metadata": {
    "id": "80efef2b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/20 19:34:54 WARN DAGScheduler: Broadcasting large task binary with size 1091.5 KiB\n",
      "23/01/20 19:34:54 WARN Instrumentation: [ce702cde] All labels are the same value and fitIntercept=true, so the coefficients will be zeros. Training is not needed.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "## Fitting the model\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=10)\n",
    "lrModel = lr.fit(train_df)\n",
    "lrPreds = lrModel.transform(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "770aa79f",
   "metadata": {
    "id": "770aa79f",
    "outputId": "20fe4ca2-eb80-4254-daa8-4477ac0b3c2f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/20 19:34:55 WARN DAGScheduler: Broadcasting large task binary with size 1111.7 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression is = 1\n"
     ]
    }
   ],
   "source": [
    "## Evaluating the model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "lr_accuracy = evaluator.evaluate(lrPreds)\n",
    "print(\"Accuracy of Logistic Regression is = %g\"% (lr_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900ccda5",
   "metadata": {
    "id": "900ccda5"
   },
   "source": [
    "## Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6523a330",
   "metadata": {
    "id": "6523a330"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/20 19:34:57 WARN DAGScheduler: Broadcasting large task binary with size 1091.0 KiB\n",
      "23/01/20 19:34:57 WARN DAGScheduler: Broadcasting large task binary with size 1091.0 KiB\n",
      "23/01/20 19:34:57 WARN DAGScheduler: Broadcasting large task binary with size 1756.2 KiB\n",
      "23/01/20 19:34:59 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "## Fitting the model\n",
    "#https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.classification.DecisionTreeClassifier.html\n",
    "dt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'label', maxDepth = 3)\n",
    "dtModel = dt.fit(train_df)\n",
    "dtPreds = dtModel.transform(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9272dae8",
   "metadata": {
    "id": "9272dae8",
    "outputId": "fe89bdfd-0dbb-431c-d3cc-d8b51760e484"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Decision Trees is = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/20 19:35:01 WARN DAGScheduler: Broadcasting large task binary with size 1109.4 KiB\n"
     ]
    }
   ],
   "source": [
    "## Evaluating the model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "dt_accuracy = evaluator.evaluate(dtPreds)\n",
    "#Accuracy of Decision Tree\n",
    "print(\"Accuracy of Decision Trees is = %g\"% (dt_accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8c9ff7",
   "metadata": {
    "id": "0a8c9ff7"
   },
   "source": [
    "# Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b415fcb5",
   "metadata": {
    "id": "b415fcb5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/20 19:35:02 WARN DAGScheduler: Broadcasting large task binary with size 1095.4 KiB\n"
     ]
    }
   ],
   "source": [
    "#applying NaiveBays algorithm\n",
    "nb = NaiveBayes(modelType=\"multinomial\",labelCol=\"label\", featuresCol=\"features\")\n",
    "nbModel = nb.fit(train_df)\n",
    "#get the prediction by transforming the model\n",
    "nb_predictions = nbModel.transform(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "962b810e",
   "metadata": {
    "id": "962b810e",
    "outputId": "0e288f8b-d39f-4d5d-8dc1-c2da3b0d26a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       0.0|    0|(65536,[2464,1430...|\n",
      "|       0.0|    0|(65536,[1903,4529...|\n",
      "|       0.0|    0|(65536,[1903,9859...|\n",
      "|       0.0|    0|(65536,[12806,230...|\n",
      "|       0.0|    0|(65536,[14013,171...|\n",
      "|       0.0|    0|(65536,[22076,344...|\n",
      "|       0.0|    0|(65536,[22351,260...|\n",
      "|       0.0|    0|(65536,[8741,2418...|\n",
      "|       0.0|    0|(65536,[13712,159...|\n",
      "|       0.0|    0|(65536,[2762,6040...|\n",
      "|       0.0|    0|(65536,[5827,8449...|\n",
      "|       0.0|    0|(65536,[7173,3399...|\n",
      "|       0.0|    0|(65536,[2548,2888...|\n",
      "|       0.0|    0|(65536,[31448,478...|\n",
      "|       0.0|    0|(65536,[308,13889...|\n",
      "|       0.0|    0|(65536,[6040,6122...|\n",
      "|       0.0|    0|(65536,[1198,4207...|\n",
      "|       0.0|    0|(65536,[2338,4166...|\n",
      "|       0.0|    0|(65536,[3386,2202...|\n",
      "|       0.0|    0|(65536,[9859,1298...|\n",
      "|       0.0|    0|(65536,[7194,1908...|\n",
      "|       0.0|    0|(65536,[6042,8923...|\n",
      "|       0.0|    0|(65536,[9859,1573...|\n",
      "|       0.0|    0|(65536,[338,4570,...|\n",
      "|       0.0|    0|(65536,[18312,264...|\n",
      "|       0.0|    0|(65536,[9347,1107...|\n",
      "|       0.0|    0|(65536,[6498,8789...|\n",
      "|       0.0|    0|(65536,[8461,8538...|\n",
      "|       0.0|    0|(65536,[389,1198,...|\n",
      "|       0.0|    0|(65536,[18184,218...|\n",
      "|       0.0|    0|(65536,[44279],[0...|\n",
      "|       0.0|    0|(65536,[44279],[0...|\n",
      "|       0.0|    0|(65536,[6052,3468...|\n",
      "|       0.0|    0|(65536,[3656,1448...|\n",
      "|       0.0|    0|(65536,[1968,6366...|\n",
      "|       0.0|    0|(65536,[6964,1818...|\n",
      "|       0.0|    0|(65536,[1602,3053...|\n",
      "|       0.0|    0|(65536,[8079,2417...|\n",
      "|       0.0|    0|(65536,[7173,3906...|\n",
      "|       0.0|    0|(65536,[6964,2635...|\n",
      "|       0.0|    0|(65536,[3861,7194...|\n",
      "|       0.0|    0|(65536,[1198,2549...|\n",
      "|       0.0|    0|(65536,[9012,1079...|\n",
      "|       0.0|    0|(65536,[1797,1123...|\n",
      "|       0.0|    0|(65536,[22105,310...|\n",
      "|       0.0|    0|(65536,[338,3639,...|\n",
      "|       0.0|    0|(65536,[737,13781...|\n",
      "|       0.0|    0|(65536,[29129,368...|\n",
      "|       0.0|    0|(65536,[22137,328...|\n",
      "|       0.0|    0|(65536,[3894,1762...|\n",
      "|       0.0|    0|(65536,[2606,9129...|\n",
      "|       0.0|    0|(65536,[7173,3356...|\n",
      "|       0.0|    0|(65536,[106,17625...|\n",
      "|       0.0|    0|(65536,[518,8031,...|\n",
      "|       0.0|    0|(65536,[12086,188...|\n",
      "|       0.0|    0|(65536,[17603,240...|\n",
      "|       0.0|    0|(65536,[2062,1035...|\n",
      "|       0.0|    0|(65536,[16967,218...|\n",
      "|       0.0|    0|(65536,[1198,1235...|\n",
      "|       0.0|    0|(65536,[1706,3681...|\n",
      "|       0.0|    0|(65536,[32,3053,1...|\n",
      "|       0.0|    0|(65536,[8317,3205...|\n",
      "|       0.0|    0|(65536,[6261,1541...|\n",
      "|       0.0|    0|(65536,[6036,9084...|\n",
      "|       0.0|    0|(65536,[8936,2365...|\n",
      "|       0.0|    0|(65536,[1198,7713...|\n",
      "|       0.0|    0|(65536,[8317,1055...|\n",
      "|       0.0|    0|(65536,[11136,201...|\n",
      "|       0.0|    0|(65536,[11650,163...|\n",
      "|       0.0|    0|(65536,[15148,173...|\n",
      "|       0.0|    0|(65536,[6781,7014...|\n",
      "|       0.0|    0|(65536,[4570,2565...|\n",
      "|       0.0|    0|(65536,[2458,1514...|\n",
      "|       0.0|    0|(65536,[7062,2007...|\n",
      "|       0.0|    0|(65536,[47153,558...|\n",
      "|       0.0|    0|(65536,[2808,5914...|\n",
      "|       0.0|    0|(65536,[24651,291...|\n",
      "|       0.0|    0|(65536,[2973,2142...|\n",
      "|       0.0|    0|(65536,[3861,1165...|\n",
      "|       0.0|    0|(65536,[7823,8408...|\n",
      "|       0.0|    0|(65536,[1602,6905...|\n",
      "|       0.0|    0|(65536,[15139,153...|\n",
      "|       0.0|    0|(65536,[1124,3098...|\n",
      "|       0.0|    0|(65536,[15295,154...|\n",
      "|       0.0|    0|(65536,[7129,8538...|\n",
      "|       0.0|    0|(65536,[3694,3612...|\n",
      "|       0.0|    0|(65536,[2268,2306...|\n",
      "|       0.0|    0|(65536,[1328,4114...|\n",
      "|       0.0|    0|(65536,[6036,1789...|\n",
      "|       0.0|    0|(65536,[2710,1411...|\n",
      "|       0.0|    0|(65536,[55666,588...|\n",
      "|       0.0|    0|(65536,[1198,1589...|\n",
      "|       0.0|    0|(65536,[518,9029,...|\n",
      "|       0.0|    0|(65536,[1589,2892...|\n",
      "|       0.0|    0|(65536,[1257,9704...|\n",
      "|       0.0|    0|(65536,[2408,1079...|\n",
      "|       0.0|    0|(65536,[4628,5879...|\n",
      "|       0.0|    0|(65536,[17893,482...|\n",
      "|       0.0|    0|(65536,[921,1689,...|\n",
      "|       0.0|    0|(65536,[2437,1313...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/20 19:35:02 WARN DAGScheduler: Broadcasting large task binary with size 1606.6 KiB\n"
     ]
    }
   ],
   "source": [
    "nb_predictions.select(\"prediction\", \"label\", \"features\").show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1781118c",
   "metadata": {
    "id": "1781118c",
    "outputId": "2abae7e7-7332-42f1-8c20-8e0d9b8f182f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of NaiveBayes is = 1\n",
      "Test Error of NaiveBayes = 0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/20 19:35:03 WARN DAGScheduler: Broadcasting large task binary with size 1617.9 KiB\n"
     ]
    }
   ],
   "source": [
    "#evaluate and print the accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "nb_accuracy = evaluator.evaluate(nb_predictions)\n",
    "print(\"Accuracy of NaiveBayes is = %g\"% (nb_accuracy))\n",
    "print(\"Test Error of NaiveBayes = %g \" % (1.0 - nb_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "56a4cd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27db55ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab1_NLP-Sentiments_Analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
