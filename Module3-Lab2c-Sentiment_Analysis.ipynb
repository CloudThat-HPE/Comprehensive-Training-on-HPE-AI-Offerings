{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0cc588f",
   "metadata": {
    "id": "c0cc588f"
   },
   "source": [
    "### Sentiment Analysis\n",
    "\n",
    "Sentiment analysis is also known as opinion mining. Sentiment analysis is a type of text mining that finds and extracts subjective information from source material, assisting businesses in determining the social sentiment associated with their brand, product, or service while monitoring online discussions.<br>\n",
    "will create machine learning model using natural language processing implemented by PySpark on Jupyter<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91df8baa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "91df8baa",
    "outputId": "58927452-94f9-4425-b851-ab941977fb6b"
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614a2e21",
   "metadata": {
    "id": "614a2e21"
   },
   "source": [
    "Starting the Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b821bf6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/16 18:22:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "#create SparkSession instance\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master('local[5]').config('spark.driver.memory','16g').appName('sentanaly').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f0151f",
   "metadata": {
    "id": "b4f0151f"
   },
   "source": [
    "Import Important modules required "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a971d950",
   "metadata": {
    "id": "a971d950"
   },
   "outputs": [],
   "source": [
    "#importing pyspark ml sql features\n",
    "from pyspark.ml import Pipeline \n",
    "from pyspark.ml.feature import CountVectorizer,StringIndexer, RegexTokenizer,StopWordsRemover\n",
    "from pyspark.sql.functions import col, udf,regexp_replace,isnull\n",
    "from pyspark.sql.types import StringType,IntegerType\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ae8f05",
   "metadata": {
    "id": "a0ae8f05"
   },
   "source": [
    "This is the sentiment140 dataset. It contains 1,600,000 tweets extracted using the twitter api.<br>\n",
    "It contains the following 6 fields:\n",
    "1. target: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
    "2. ids: The id of the tweet ( 2087)\n",
    "3. date: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
    "4. flag: The query (lyx). If there is no query, then this value is NO_QUERY.\n",
    "5. user: the user that tweeted (robotickilldozr)\n",
    "6. text: the text of the tweet (Lyx is cool)<br>\n",
    "Now we are loading the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "25ec274d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "25ec274d",
    "outputId": "27789d30-a43b-4f58-8bda-5d46ce75748f"
   },
   "outputs": [],
   "source": [
    "#read the csv containing twitter data\n",
    "news_data = spark.read.csv('trainingsentimentdata.csv',header= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "47f77ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "news_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e5a69587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "|_c0|       _c1|                 _c2|     _c3|            _c4|                 _c5|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "|  0|1467810369|Mon Apr 06 22:19:...|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
      "|  0|1467810672|Mon Apr 06 22:19:...|NO_QUERY|  scotthamilton|is upset that he ...|\n",
      "|  0|1467810917|Mon Apr 06 22:19:...|NO_QUERY|       mattycus|@Kenichan I dived...|\n",
      "|  0|1467811184|Mon Apr 06 22:19:...|NO_QUERY|        ElleCTF|my whole body fee...|\n",
      "|  0|1467811193|Mon Apr 06 22:19:...|NO_QUERY|         Karoli|@nationwideclass ...|\n",
      "|  0|1467811372|Mon Apr 06 22:20:...|NO_QUERY|       joy_wolf|@Kwesidei not the...|\n",
      "|  0|1467811592|Mon Apr 06 22:20:...|NO_QUERY|        mybirch|         Need a hug |\n",
      "|  0|1467811594|Mon Apr 06 22:20:...|NO_QUERY|           coZZ|@LOLTrish hey  lo...|\n",
      "|  0|1467811795|Mon Apr 06 22:20:...|NO_QUERY|2Hood4Hollywood|@Tatiana_K nope t...|\n",
      "|  0|1467812025|Mon Apr 06 22:20:...|NO_QUERY|        mimismo|@twittera que me ...|\n",
      "|  0|1467812416|Mon Apr 06 22:20:...|NO_QUERY| erinx3leannexo|spring break in p...|\n",
      "|  0|1467812579|Mon Apr 06 22:20:...|NO_QUERY|   pardonlauren|I just re-pierced...|\n",
      "|  0|1467812723|Mon Apr 06 22:20:...|NO_QUERY|           TLeC|@caregiving I cou...|\n",
      "|  0|1467812771|Mon Apr 06 22:20:...|NO_QUERY|robrobbierobert|@octolinz16 It it...|\n",
      "|  0|1467812784|Mon Apr 06 22:20:...|NO_QUERY|    bayofwolves|@smarrison i woul...|\n",
      "|  0|1467812799|Mon Apr 06 22:20:...|NO_QUERY|     HairByJess|@iamjazzyfizzle I...|\n",
      "|  0|1467812964|Mon Apr 06 22:20:...|NO_QUERY| lovesongwriter|Hollis' death sce...|\n",
      "|  0|1467813137|Mon Apr 06 22:20:...|NO_QUERY|       armotley|about to file taxes |\n",
      "|  0|1467813579|Mon Apr 06 22:20:...|NO_QUERY|     starkissed|@LettyA ahh ive a...|\n",
      "|  0|1467813782|Mon Apr 06 22:20:...|NO_QUERY|      gi_gi_bee|@FakerPattyPattz ...|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#printing the data\n",
    "news_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d346dee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.cache of DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string]>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#news_data = news_data.limit(500)\n",
    "#news_data.cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a520a22",
   "metadata": {
    "id": "2a520a22"
   },
   "source": [
    "We can check the count of total items in the dataset for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ca4fcc97",
   "metadata": {
    "id": "ca4fcc97",
    "outputId": "97d5acd1-c2d0-43a7-f432-836814b25368"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1600000"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count the rows of dataset\n",
    "news_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00df7f9c",
   "metadata": {
    "id": "00df7f9c"
   },
   "source": [
    "We are selecting the titles of tweets and the corresponding category of each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7bab7451",
   "metadata": {
    "id": "7bab7451",
    "outputId": "f5200575-50c3-42bc-ac1a-cc428074ba04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|                 _c5|_c0|\n",
      "+--------------------+---+\n",
      "|@switchfoot http:...|  0|\n",
      "|is upset that he ...|  0|\n",
      "|@Kenichan I dived...|  0|\n",
      "|my whole body fee...|  0|\n",
      "|@nationwideclass ...|  0|\n",
      "|@Kwesidei not the...|  0|\n",
      "|         Need a hug |  0|\n",
      "|@LOLTrish hey  lo...|  0|\n",
      "|@Tatiana_K nope t...|  0|\n",
      "|@twittera que me ...|  0|\n",
      "|spring break in p...|  0|\n",
      "|I just re-pierced...|  0|\n",
      "|@caregiving I cou...|  0|\n",
      "|@octolinz16 It it...|  0|\n",
      "|@smarrison i woul...|  0|\n",
      "|@iamjazzyfizzle I...|  0|\n",
      "|Hollis' death sce...|  0|\n",
      "|about to file taxes |  0|\n",
      "|@LettyA ahh ive a...|  0|\n",
      "|@FakerPattyPattz ...|  0|\n",
      "+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#pick only _c5 and _c0 columns and place in title_category\n",
    "title_category = news_data.select(\"_c5\",\"_c0\")\n",
    "title_category.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd72e0d",
   "metadata": {
    "id": "cfd72e0d"
   },
   "source": [
    "This is the custom function definition to count the null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "748d0cef",
   "metadata": {
    "id": "748d0cef"
   },
   "outputs": [],
   "source": [
    "#function to count null values in the columns\n",
    "def null_value_count(df):\n",
    "  null_columns_counts = [] #initialize array to null\n",
    "  numRows = df.count() #count the number of rows\n",
    "  for k in df.columns:\n",
    "    nullRows = df.where(col(k).isNull()).count() #count null rows\n",
    "    if(nullRows > 0):\n",
    "      temp = k,nullRows\n",
    "      null_columns_counts.append(temp)\n",
    "  return(null_columns_counts) #return count of null collumns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeaa0a96",
   "metadata": {
    "id": "aeaa0a96"
   },
   "source": [
    "We are applying the custom function to the data frsme title_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "11b9d67a",
   "metadata": {
    "id": "11b9d67a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_columns_count_list = null_value_count(title_category)\n",
    "#spark.createDataFrame(null_columns_count_list, ['Column_With_Null_Value', 'Null_Values_Count']).show()\n",
    "null_columns_count_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1840e9",
   "metadata": {
    "id": "3b1840e9"
   },
   "source": [
    "# Cleaning the dataset\n",
    "\n",
    "Now we can drop the null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "534b2a42",
   "metadata": {
    "id": "534b2a42",
    "outputId": "e2b9fef4-4103-4680-dc9a-43d658740cb6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------+---+\n",
      "|_c5                                                                                                                  |_c0|\n",
      "+---------------------------------------------------------------------------------------------------------------------+---+\n",
      "|@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  |0  |\n",
      "|is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!      |0  |\n",
      "|@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                            |0  |\n",
      "|my whole body feels itchy and like its on fire                                                                       |0  |\n",
      "|@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.       |0  |\n",
      "|@Kwesidei not the whole crew                                                                                         |0  |\n",
      "|Need a hug                                                                                                           |0  |\n",
      "|@LOLTrish hey  long time no see! Yes.. Rains a bit ,only a bit  LOL , I'm fine thanks , how's you ?                  |0  |\n",
      "|@Tatiana_K nope they didn't have it                                                                                  |0  |\n",
      "|@twittera que me muera ?                                                                                             |0  |\n",
      "|spring break in plain city... it's snowing                                                                           |0  |\n",
      "|I just re-pierced my ears                                                                                            |0  |\n",
      "|@caregiving I couldn't bear to watch it.  And I thought the UA loss was embarrassing . . . . .                       |0  |\n",
      "|@octolinz16 It it counts, idk why I did either. you never talk to me anymore                                         |0  |\n",
      "|@smarrison i would've been the first, but i didn't have a gun.    not really though, zac snyder's just a doucheclown.|0  |\n",
      "|@iamjazzyfizzle I wish I got to watch it with you!! I miss you and @iamlilnicki  how was the premiere?!              |0  |\n",
      "|Hollis' death scene will hurt me severely to watch on film  wry is directors cut not out now?                        |0  |\n",
      "|about to file taxes                                                                                                  |0  |\n",
      "|@LettyA ahh ive always wanted to see rent  love the soundtrack!!                                                     |0  |\n",
      "|@FakerPattyPattz Oh dear. Were you drinking out of the forgotten table drinks?                                       |0  |\n",
      "+---------------------------------------------------------------------------------------------------------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#drop not applicable and null values from the category\n",
    "title_category = title_category.dropna()\n",
    "title_category.count()\n",
    "title_category.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c86548d4",
   "metadata": {
    "id": "c86548d4",
    "outputId": "2e945b46-fe60-443c-a180-e647f58ca78d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Tweets: string (nullable = true)\n",
      " |-- Sentiment: string (nullable = true)\n",
      "\n",
      "+--------------------+---------+\n",
      "|              Tweets|Sentiment|\n",
      "+--------------------+---------+\n",
      "|@switchfoot http:...|        0|\n",
      "|is upset that he ...|        0|\n",
      "|@Kenichan I dived...|        0|\n",
      "|my whole body fee...|        0|\n",
      "|@nationwideclass ...|        0|\n",
      "|@Kwesidei not the...|        0|\n",
      "|         Need a hug |        0|\n",
      "|@LOLTrish hey  lo...|        0|\n",
      "|@Tatiana_K nope t...|        0|\n",
      "|@twittera que me ...|        0|\n",
      "|spring break in p...|        0|\n",
      "|I just re-pierced...|        0|\n",
      "|@caregiving I cou...|        0|\n",
      "|@octolinz16 It it...|        0|\n",
      "|@smarrison i woul...|        0|\n",
      "|@iamjazzyfizzle I...|        0|\n",
      "|Hollis' death sce...|        0|\n",
      "|about to file taxes |        0|\n",
      "|@LettyA ahh ive a...|        0|\n",
      "|@FakerPattyPattz ...|        0|\n",
      "+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "#data containing from the csv\n",
    "oldColumns = title_category.schema.names\n",
    "#creating new columns with heading 'Tweets' and 'Sentiment'\n",
    "newColumns = ['Tweets','Sentiment']\n",
    "\n",
    "#applying lambda function to create a copy of old columns to new columns\n",
    "title_category = reduce(lambda title_category, idx: title_category.withColumnRenamed(oldColumns[idx], newColumns[idx]),range(len(oldColumns)), title_category)\n",
    "title_category.printSchema()\n",
    "title_category.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad4ef83",
   "metadata": {
    "id": "8ad4ef83"
   },
   "source": [
    "Now we can remove the numbers in tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1e67a8cd",
   "metadata": {
    "id": "1e67a8cd",
    "outputId": "2988a0ce-a75e-44ea-e41b-868f04362dbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------+\n",
      "|Tweets                                                                                                               |only_str                                                                                                             |\n",
      "+---------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------+\n",
      "|@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  |@switchfoot http://twitpic.com/yzl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D    |\n",
      "|is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!      |is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!      |\n",
      "|@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                            |@Kenichan I dived many times for the ball. Managed to save %  The rest go out of bounds                              |\n",
      "|my whole body feels itchy and like its on fire                                                                       |my whole body feels itchy and like its on fire                                                                       |\n",
      "|@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.       |@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.       |\n",
      "|@Kwesidei not the whole crew                                                                                         |@Kwesidei not the whole crew                                                                                         |\n",
      "|Need a hug                                                                                                           |Need a hug                                                                                                           |\n",
      "|@LOLTrish hey  long time no see! Yes.. Rains a bit ,only a bit  LOL , I'm fine thanks , how's you ?                  |@LOLTrish hey  long time no see! Yes.. Rains a bit ,only a bit  LOL , I'm fine thanks , how's you ?                  |\n",
      "|@Tatiana_K nope they didn't have it                                                                                  |@Tatiana_K nope they didn't have it                                                                                  |\n",
      "|@twittera que me muera ?                                                                                             |@twittera que me muera ?                                                                                             |\n",
      "|spring break in plain city... it's snowing                                                                           |spring break in plain city... it's snowing                                                                           |\n",
      "|I just re-pierced my ears                                                                                            |I just re-pierced my ears                                                                                            |\n",
      "|@caregiving I couldn't bear to watch it.  And I thought the UA loss was embarrassing . . . . .                       |@caregiving I couldn't bear to watch it.  And I thought the UA loss was embarrassing . . . . .                       |\n",
      "|@octolinz16 It it counts, idk why I did either. you never talk to me anymore                                         |@octolinz It it counts, idk why I did either. you never talk to me anymore                                           |\n",
      "|@smarrison i would've been the first, but i didn't have a gun.    not really though, zac snyder's just a doucheclown.|@smarrison i would've been the first, but i didn't have a gun.    not really though, zac snyder's just a doucheclown.|\n",
      "|@iamjazzyfizzle I wish I got to watch it with you!! I miss you and @iamlilnicki  how was the premiere?!              |@iamjazzyfizzle I wish I got to watch it with you!! I miss you and @iamlilnicki  how was the premiere?!              |\n",
      "|Hollis' death scene will hurt me severely to watch on film  wry is directors cut not out now?                        |Hollis' death scene will hurt me severely to watch on film  wry is directors cut not out now?                        |\n",
      "|about to file taxes                                                                                                  |about to file taxes                                                                                                  |\n",
      "|@LettyA ahh ive always wanted to see rent  love the soundtrack!!                                                     |@LettyA ahh ive always wanted to see rent  love the soundtrack!!                                                     |\n",
      "|@FakerPattyPattz Oh dear. Were you drinking out of the forgotten table drinks?                                       |@FakerPattyPattz Oh dear. Were you drinking out of the forgotten table drinks?                                       |\n",
      "+---------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#cleaning the numbers from tweets\n",
    "title_category = title_category.withColumn(\"only_str\",regexp_replace(col('Tweets'), '\\d+', ''))\n",
    "title_category.select(\"Tweets\",\"only_str\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49f034f",
   "metadata": {
    "id": "b49f034f"
   },
   "source": [
    "Split the text into constituent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1dae25af",
   "metadata": {
    "id": "1dae25af",
    "outputId": "53236734-9acb-46f6-c0aa-6ece40608478"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------------------+--------------------+\n",
      "|              Tweets|Sentiment|            only_str|               words|\n",
      "+--------------------+---------+--------------------+--------------------+\n",
      "|@switchfoot http:...|        0|@switchfoot http:...|[switchfoot, http...|\n",
      "|is upset that he ...|        0|is upset that he ...|[is, upset, that,...|\n",
      "|@Kenichan I dived...|        0|@Kenichan I dived...|[kenichan, i, div...|\n",
      "|my whole body fee...|        0|my whole body fee...|[my, whole, body,...|\n",
      "|@nationwideclass ...|        0|@nationwideclass ...|[nationwideclass,...|\n",
      "|@Kwesidei not the...|        0|@Kwesidei not the...|[kwesidei, not, t...|\n",
      "|         Need a hug |        0|         Need a hug |      [need, a, hug]|\n",
      "|@LOLTrish hey  lo...|        0|@LOLTrish hey  lo...|[loltrish, hey, l...|\n",
      "|@Tatiana_K nope t...|        0|@Tatiana_K nope t...|[tatiana_k, nope,...|\n",
      "|@twittera que me ...|        0|@twittera que me ...|[twittera, que, m...|\n",
      "|spring break in p...|        0|spring break in p...|[spring, break, i...|\n",
      "|I just re-pierced...|        0|I just re-pierced...|[i, just, re, pie...|\n",
      "|@caregiving I cou...|        0|@caregiving I cou...|[caregiving, i, c...|\n",
      "|@octolinz16 It it...|        0|@octolinz It it c...|[octolinz, it, it...|\n",
      "|@smarrison i woul...|        0|@smarrison i woul...|[smarrison, i, wo...|\n",
      "|@iamjazzyfizzle I...|        0|@iamjazzyfizzle I...|[iamjazzyfizzle, ...|\n",
      "|Hollis' death sce...|        0|Hollis' death sce...|[hollis, death, s...|\n",
      "|about to file taxes |        0|about to file taxes |[about, to, file,...|\n",
      "|@LettyA ahh ive a...|        0|@LettyA ahh ive a...|[lettya, ahh, ive...|\n",
      "|@FakerPattyPattz ...|        0|@FakerPattyPattz ...|[fakerpattypattz,...|\n",
      "+--------------------+---------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#split the text to words or tokens\n",
    "regex_tokenizer = RegexTokenizer(inputCol=\"only_str\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "raw_words = regex_tokenizer.transform(title_category)\n",
    "raw_words.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1526619",
   "metadata": {
    "id": "d1526619"
   },
   "source": [
    "Remove the stop words from segregated list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "94d5434a",
   "metadata": {
    "id": "94d5434a",
    "outputId": "fcd3c94d-5199-4091-e677-d8980b7d2431"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------+\n",
      "|words                                                                                                                                |filtered                                                                                     |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------+\n",
      "|[switchfoot, http, twitpic, com, yzl, awww, that, s, a, bummer, you, shoulda, got, david, carr, of, third, day, to, do, it, d]       |[switchfoot, http, twitpic, com, yzl, awww, bummer, shoulda, got, david, carr, third, day, d]|\n",
      "|[is, upset, that, he, can, t, update, his, facebook, by, texting, it, and, might, cry, as, a, result, school, today, also, blah]     |[upset, update, facebook, texting, might, cry, result, school, today, also, blah]            |\n",
      "|[kenichan, i, dived, many, times, for, the, ball, managed, to, save, the, rest, go, out, of, bounds]                                 |[kenichan, dived, many, times, ball, managed, save, rest, go, bounds]                        |\n",
      "|[my, whole, body, feels, itchy, and, like, its, on, fire]                                                                            |[whole, body, feels, itchy, like, fire]                                                      |\n",
      "|[nationwideclass, no, it, s, not, behaving, at, all, i, m, mad, why, am, i, here, because, i, can, t, see, you, all, over, there]    |[nationwideclass, behaving, m, mad, see]                                                     |\n",
      "|[kwesidei, not, the, whole, crew]                                                                                                    |[kwesidei, whole, crew]                                                                      |\n",
      "|[need, a, hug]                                                                                                                       |[need, hug]                                                                                  |\n",
      "|[loltrish, hey, long, time, no, see, yes, rains, a, bit, only, a, bit, lol, i, m, fine, thanks, how, s, you]                         |[loltrish, hey, long, time, see, yes, rains, bit, bit, lol, m, fine, thanks]                 |\n",
      "|[tatiana_k, nope, they, didn, t, have, it]                                                                                           |[tatiana_k, nope, didn]                                                                      |\n",
      "|[twittera, que, me, muera]                                                                                                           |[twittera, que, muera]                                                                       |\n",
      "|[spring, break, in, plain, city, it, s, snowing]                                                                                     |[spring, break, plain, city, snowing]                                                        |\n",
      "|[i, just, re, pierced, my, ears]                                                                                                     |[re, pierced, ears]                                                                          |\n",
      "|[caregiving, i, couldn, t, bear, to, watch, it, and, i, thought, the, ua, loss, was, embarrassing]                                   |[caregiving, couldn, bear, watch, thought, ua, loss, embarrassing]                           |\n",
      "|[octolinz, it, it, counts, idk, why, i, did, either, you, never, talk, to, me, anymore]                                              |[octolinz, counts, idk, either, never, talk, anymore]                                        |\n",
      "|[smarrison, i, would, ve, been, the, first, but, i, didn, t, have, a, gun, not, really, though, zac, snyder, s, just, a, doucheclown]|[smarrison, ve, first, didn, gun, really, though, zac, snyder, doucheclown]                  |\n",
      "|[iamjazzyfizzle, i, wish, i, got, to, watch, it, with, you, i, miss, you, and, iamlilnicki, how, was, the, premiere]                 |[iamjazzyfizzle, wish, got, watch, miss, iamlilnicki, premiere]                              |\n",
      "|[hollis, death, scene, will, hurt, me, severely, to, watch, on, film, wry, is, directors, cut, not, out, now]                        |[hollis, death, scene, hurt, severely, watch, film, wry, directors, cut]                     |\n",
      "|[about, to, file, taxes]                                                                                                             |[file, taxes]                                                                                |\n",
      "|[lettya, ahh, ive, always, wanted, to, see, rent, love, the, soundtrack]                                                             |[lettya, ahh, ive, always, wanted, see, rent, love, soundtrack]                              |\n",
      "|[fakerpattypattz, oh, dear, were, you, drinking, out, of, the, forgotten, table, drinks]                                             |[fakerpattypattz, oh, dear, drinking, forgotten, table, drinks]                              |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Removing the stop words from the list of words\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "words_df = remover.transform(raw_words)\n",
    "words_df.select(\"words\",\"filtered\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e215cd",
   "metadata": {
    "id": "10e215cd"
   },
   "source": [
    "Convert text into vectors of token counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff14dc4",
   "metadata": {
    "id": "7ff14dc4"
   },
   "source": [
    "# Partition the dataset into training and test datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a1efd33e",
   "metadata": {
    "id": "a1efd33e",
    "outputId": "5c763c58-2bd4-465c-8721-b09bd961d5f8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------------------+--------------------+--------------------+\n",
      "|              Tweets|Sentiment|            only_str|               words|            filtered|\n",
      "+--------------------+---------+--------------------+--------------------+--------------------+\n",
      "|                 ...|        0|                 ...|[i, missed, the, ...|[missed, new, moo...|\n",
      "|           FUCK YOU!|        0|           FUCK YOU!|         [fuck, you]|              [fuck]|\n",
      "|          i want ...|        0|          i want ...|[i, want, some, b...|[want, ben, amp, ...|\n",
      "|        my head f...|        0|        my head f...|[my, head, feels,...|[head, feels, lik...|\n",
      "|        my heart ...|        0|        my heart ...|[my, heart, hurts...|[heart, hurts, ba...|\n",
      "|      this weeken...|        0|      this weeken...|[this, weekend, h...|[weekend, sucked,...|\n",
      "|            #canucks|        0|            #canucks|           [canucks]|           [canucks]|\n",
      "|     &lt;- but mu...|        0|     &lt;- but mu...|[lt, but, mustach...|[lt, mustache, ma...|\n",
      "|     I dont like ...|        0|     I dont like ...|[i, dont, like, t...|[dont, like, week...|\n",
      "|     I'll get on ...|        0|     I'll get on ...|[i, ll, get, on, ...|[ll, get, right, ...|\n",
      "|     what the fuc...|        0|     what the fuc...|[what, the, fuccc...|  [fucccckkkkkkkkkk]|\n",
      "|    Not feeling i...|        0|    Not feeling i...|[not, feeling, it...|[feeling, today, ...|\n",
      "|    awhhe man.......|        0|    awhhe man.......|[awhhe, man, i, m...|[awhhe, man, m, c...|\n",
      "|    on the comput...|        0|    on the comput...|[on, the, compute...|[computer, ill, l...|\n",
      "|   (must i say mo...|        0|   (must i say mo...|[must, i, say, more]|         [must, say]|\n",
      "|   *old me's dead...|        0|   *old me's dead...|[old, me, s, dead...|   [old, dead, gone]|\n",
      "|   ...  Headed to...|        0|   ...  Headed to...|[headed, to, hosp...|[headed, hospitol...|\n",
      "|   Awwwwwh  i wan...|        0|   Awwwwwh  i wan...|[awwwwwh, i, want...|[awwwwwh, wanted,...|\n",
      "|   Awwwwwh  i wan...|        0|   Awwwwwh  i wan...|[awwwwwh, i, want...|[awwwwwh, wanted,...|\n",
      "|   Boston Globe c...|        0|   Boston Globe c...|[boston, globe, c...|[boston, globe, s...|\n",
      "+--------------------+---------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 86:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------------------+--------------------+--------------------+\n",
      "|              Tweets|Sentiment|            only_str|               words|            filtered|\n",
      "+--------------------+---------+--------------------+--------------------+--------------------+\n",
      "|       i really2 ...|        0|       i really d...|[i, really, don, ...|[really, like, co...|\n",
      "|      My current ...|        0|      My current ...|[my, current, hea...|[current, headset...|\n",
      "|               angry|        0|               angry|             [angry]|             [angry]|\n",
      "|     jb isnt show...|        0|     jb isnt show...|[jb, isnt, showin...|[jb, isnt, showin...|\n",
      "|     ok thats it ...|        0|     ok thats it ...|[ok, thats, it, y...|    [ok, thats, win]|\n",
      "|     ...lonely night|        0|     ...lonely night|     [lonely, night]|     [lonely, night]|\n",
      "|    I just cut my...|        0|    I just cut my...|[i, just, cut, my...|[cut, beard, grow...|\n",
      "|       wompppp wompp|        0|       wompppp wompp|    [wompppp, wompp]|    [wompppp, wompp]|\n",
      "|   BoRinG   ): wh...|        0|   BoRinG   ): wh...|[boring, whats, w...|[boring, whats, w...|\n",
      "|      CRY CRY CRY   |        0|      CRY CRY CRY   |     [cry, cry, cry]|     [cry, cry, cry]|\n",
      "|   I hate it when...|        0|   I hate it when...|[i, hate, it, whe...|[hate, athlete, a...|\n",
      "|   Lost 2 followers.|        0|    Lost  followers.|   [lost, followers]|   [lost, followers]|\n",
      "|   No man is wort...|        0|   No man is wort...|[no, man, is, wor...|[man, worth, tear...|\n",
      "|         i missed it|        0|         i missed it|     [i, missed, it]|            [missed]|\n",
      "|   kinda but not ...|        0|   kinda but not ...|[kinda, but, not,...|[kinda, really, k...|\n",
      "|  I TALKED TO U B...|        0|  I TALKED TO U B...|[i, talked, to, u...|[talked, u, didnt...|\n",
      "|  I have a really...|        0|  I have a really...|[i, have, a, real...|[really, bad, mem...|\n",
      "|  I need a U2 fix...|        0|  I need a U fix ...|[i, need, a, u, f...|      [need, u, fix]|\n",
      "|  I see I missed ...|        0|  I see I missed ...|[i, see, i, misse...|[see, missed, nec...|\n",
      "|  Thanks for your...|        0|  Thanks for your...|[thanks, for, you...|[thanks, definiti...|\n",
      "+--------------------+---------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Partition the dataset into trainingData 80% and testData 20%\n",
    "(trainingData, testData) = words_df.randomSplit([0.8, 0.2],seed = 11)\n",
    "trainingData.show()\n",
    "testData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df21afb",
   "metadata": {
    "id": "1df21afb"
   },
   "source": [
    "# Model Training and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e0267532",
   "metadata": {
    "id": "e0267532",
    "outputId": "82a52fc3-bfe3-4996-c5af-acd8922e2d42"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/16 19:02:13 WARN DAGScheduler: Broadcasting large task binary with size 1082.5 KiB\n",
      "[Stage 88:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|              Tweets|Sentiment|            only_str|               words|            filtered|                  tf|            features|\n",
      "+--------------------+---------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                 ...|        0|                 ...|[i, missed, the, ...|[missed, new, moo...|(65536,[4495,2429...|(65536,[4495,2429...|\n",
      "|           FUCK YOU!|        0|           FUCK YOU!|         [fuck, you]|              [fuck]|(65536,[40503],[1...|(65536,[40503],[5...|\n",
      "|          i want ...|        0|          i want ...|[i, want, some, b...|[want, ben, amp, ...|(65536,[13007,352...|(65536,[13007,352...|\n",
      "|        my head f...|        0|        my head f...|[my, head, feels,...|[head, feels, lik...|(65536,[2548,1165...|(65536,[2548,1165...|\n",
      "|        my heart ...|        0|        my heart ...|[my, heart, hurts...|[heart, hurts, ba...|(65536,[29514,468...|(65536,[29514,468...|\n",
      "+--------------------+---------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "#tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "#HashingTF maps a sequence of terms\n",
    "hashtf = HashingTF(numFeatures=2**16, inputCol=\"filtered\", outputCol='tf')\n",
    "\n",
    "#IDF stands for Inverse Document Frequency (Common:Rare)\n",
    "idf = IDF(inputCol='tf', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "#label_stringIdx = StringIndexer(inputCol = \"target\", outputCol = \"label\")\n",
    "pipeline = Pipeline(stages=[hashtf, idf])\n",
    "\n",
    "pipelineFit = pipeline.fit(trainingData)\n",
    "train_df = pipelineFit.transform(trainingData)\n",
    "val_df = pipelineFit.transform(testData)\n",
    "train_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ae08819f",
   "metadata": {
    "id": "ae08819f"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "train_df=train_df.withColumnRenamed('Sentiment', 'label')\n",
    "train_df=train_df.withColumn(\"label\",train_df.label.cast('int'))\n",
    "val_df=val_df.withColumnRenamed('Sentiment', 'label')\n",
    "val_df=val_df.withColumn(\"label\",val_df.label.cast('int'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6a7f6b",
   "metadata": {
    "id": "3e6a7f6b"
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "80efef2b",
   "metadata": {
    "id": "80efef2b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/16 19:02:21 WARN DAGScheduler: Broadcasting large task binary with size 1086.6 KiB\n",
      "23/02/16 19:02:48 WARN DAGScheduler: Broadcasting large task binary with size 1088.1 KiB\n",
      "23/02/16 19:03:13 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "23/02/16 19:03:13 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "23/02/16 19:03:16 WARN DAGScheduler: Broadcasting large task binary with size 1088.1 KiB\n",
      "23/02/16 19:03:18 WARN DAGScheduler: Broadcasting large task binary with size 1088.1 KiB\n",
      "23/02/16 19:03:19 WARN DAGScheduler: Broadcasting large task binary with size 1088.1 KiB\n",
      "23/02/16 19:03:20 WARN DAGScheduler: Broadcasting large task binary with size 1088.1 KiB\n",
      "23/02/16 19:03:21 WARN DAGScheduler: Broadcasting large task binary with size 1088.1 KiB\n",
      "23/02/16 19:03:22 WARN DAGScheduler: Broadcasting large task binary with size 1088.1 KiB\n",
      "23/02/16 19:03:23 WARN DAGScheduler: Broadcasting large task binary with size 1088.1 KiB\n",
      "23/02/16 19:03:24 WARN DAGScheduler: Broadcasting large task binary with size 1088.1 KiB\n",
      "23/02/16 19:03:25 WARN DAGScheduler: Broadcasting large task binary with size 1088.1 KiB\n",
      "23/02/16 19:03:26 WARN DAGScheduler: Broadcasting large task binary with size 1088.1 KiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "## Fitting the model\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=10)\n",
    "lrModel = lr.fit(train_df)\n",
    "lrPreds = lrModel.transform(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "770aa79f",
   "metadata": {
    "id": "770aa79f",
    "outputId": "20fe4ca2-eb80-4254-daa8-4477ac0b3c2f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/16 19:03:28 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "[Stage 101:==================================>                      (3 + 2) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression is = 0.763975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## Evaluating the model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "lr_accuracy = evaluator.evaluate(lrPreds)\n",
    "print(\"Accuracy of Logistic Regression is = %g\"% (lr_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900ccda5",
   "metadata": {
    "id": "900ccda5"
   },
   "source": [
    "## Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6523a330",
   "metadata": {
    "id": "6523a330"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/16 19:05:10 WARN DAGScheduler: Broadcasting large task binary with size 1086.1 KiB\n",
      "23/02/16 19:05:17 WARN DAGScheduler: Broadcasting large task binary with size 1086.1 KiB\n",
      "23/02/16 19:05:42 WARN DAGScheduler: Broadcasting large task binary with size 1753.2 KiB\n",
      "23/02/16 19:06:26 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/02/16 19:06:58 WARN MemoryStore: Not enough space to cache rdd_460_3 in memory! (computed 1270.7 MiB so far)\n",
      "23/02/16 19:06:58 WARN BlockManager: Persisting block rdd_460_3 to disk instead.\n",
      "23/02/16 19:06:58 WARN MemoryStore: Not enough space to cache rdd_460_0 in memory! (computed 1270.7 MiB so far)\n",
      "23/02/16 19:06:58 WARN BlockManager: Persisting block rdd_460_0 to disk instead.\n",
      "23/02/16 19:06:58 WARN MemoryStore: Not enough space to cache rdd_460_2 in memory! (computed 1270.7 MiB so far)\n",
      "23/02/16 19:06:58 WARN BlockManager: Persisting block rdd_460_2 to disk instead.\n",
      "23/02/16 19:06:59 WARN MemoryStore: Not enough space to cache rdd_460_1 in memory! (computed 1270.7 MiB so far)\n",
      "23/02/16 19:06:59 WARN BlockManager: Persisting block rdd_460_1 to disk instead.\n",
      "23/02/16 19:07:52 WARN MemoryStore: Not enough space to cache rdd_460_4 in memory! (computed 6.6 GiB so far)\n",
      "23/02/16 19:07:52 WARN BlockManager: Persisting block rdd_460_4 to disk instead.\n",
      "23/02/16 19:18:00 WARN MemoryStore: Not enough space to cache rdd_460_4 in memory! (computed 6.6 GiB so far)\n",
      "23/02/16 19:18:50 WARN MemoryStore: Not enough space to cache rdd_460_0 in memory! (computed 1907.0 MiB so far)\n",
      "23/02/16 19:18:54 WARN MemoryStore: Not enough space to cache rdd_460_2 in memory! (computed 2.8 GiB so far)\n",
      "23/02/16 19:18:54 WARN MemoryStore: Not enough space to cache rdd_460_1 in memory! (computed 2.8 GiB so far)\n",
      "23/02/16 19:18:55 WARN MemoryStore: Not enough space to cache rdd_460_3 in memory! (computed 845.8 MiB so far)\n",
      "23/02/16 19:25:46 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/02/16 19:25:52 WARN MemoryStore: Not enough space to cache rdd_460_3 in memory! (computed 1270.7 MiB so far)\n",
      "23/02/16 19:25:52 WARN MemoryStore: Not enough space to cache rdd_460_0 in memory! (computed 1270.7 MiB so far)\n",
      "23/02/16 19:25:54 WARN MemoryStore: Not enough space to cache rdd_460_4 in memory! (computed 1907.0 MiB so far)\n",
      "23/02/16 19:25:54 WARN MemoryStore: Not enough space to cache rdd_460_1 in memory! (computed 1907.0 MiB so far)\n",
      "23/02/16 19:25:55 WARN MemoryStore: Not enough space to cache rdd_460_2 in memory! (computed 1907.0 MiB so far)\n",
      "23/02/16 19:32:36 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/02/16 19:32:45 WARN MemoryStore: Not enough space to cache rdd_460_4 in memory! (computed 1270.7 MiB so far)\n",
      "23/02/16 19:32:46 WARN MemoryStore: Not enough space to cache rdd_460_2 in memory! (computed 1270.7 MiB so far)\n",
      "23/02/16 19:32:47 WARN MemoryStore: Not enough space to cache rdd_460_1 in memory! (computed 1907.0 MiB so far)\n",
      "23/02/16 19:32:47 WARN MemoryStore: Not enough space to cache rdd_460_0 in memory! (computed 1907.0 MiB so far)\n",
      "23/02/16 19:32:47 WARN MemoryStore: Not enough space to cache rdd_460_3 in memory! (computed 1907.0 MiB so far)\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "## Fitting the model\n",
    "dt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'label', maxDepth = 3)\n",
    "dtModel = dt.fit(train_df)\n",
    "dtPreds = dtModel.transform(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9272dae8",
   "metadata": {
    "id": "9272dae8",
    "outputId": "fe89bdfd-0dbb-431c-d3cc-d8b51760e484"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/16 19:39:40 WARN DAGScheduler: Broadcasting large task binary with size 1106.9 KiB\n",
      "[Stage 115:======================>                                  (2 + 3) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Decision Trees is = 0.530731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 115:==================================>                      (3 + 2) / 5]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## Evaluating the model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "dt_accuracy = evaluator.evaluate(dtPreds)\n",
    "#Accuracy of Decision Tree\n",
    "print(\"Accuracy of Decision Trees is = %g\"% (dt_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8c9ff7",
   "metadata": {
    "id": "0a8c9ff7"
   },
   "source": [
    "# Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b415fcb5",
   "metadata": {
    "id": "b415fcb5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/16 19:39:59 WARN DAGScheduler: Broadcasting large task binary with size 1091.4 KiB\n",
      "23/02/16 19:40:26 WARN DAGScheduler: Broadcasting large task binary with size 1073.4 KiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#applying NaiveBays algorithm\n",
    "nb = NaiveBayes(modelType=\"multinomial\",labelCol=\"label\", featuresCol=\"features\")\n",
    "nbModel = nb.fit(train_df)\n",
    "#get the prediction by transforming the model\n",
    "nb_predictions = nbModel.transform(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "962b810e",
   "metadata": {
    "id": "962b810e",
    "outputId": "0e288f8b-d39f-4d5d-8dc1-c2da3b0d26a1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/16 19:40:27 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "[Stage 119:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       0.0|    0|(65536,[11650,191...|\n",
      "|       0.0|    0|(65536,[1198,2001...|\n",
      "|       0.0|    0|(65536,[465],[7.4...|\n",
      "|       0.0|    0|(65536,[2284,2108...|\n",
      "|       1.0|    0|(65536,[1589,1034...|\n",
      "|       0.0|    0|(65536,[11828,588...|\n",
      "|       0.0|    0|(65536,[10372,160...|\n",
      "|       1.0|    0|(65536,[7626,3817...|\n",
      "|       0.0|    0|(65536,[4570,4832...|\n",
      "|       0.0|    0|(65536,[65018],[1...|\n",
      "|       0.0|    0|(65536,[6083,7173...|\n",
      "|       0.0|    0|(65536,[19153,605...|\n",
      "|       0.0|    0|(65536,[21823,241...|\n",
      "|       0.0|    0|(65536,[34288],[4...|\n",
      "|       0.0|    0|(65536,[32656,417...|\n",
      "|       0.0|    0|(65536,[2331,2743...|\n",
      "|       0.0|    0|(65536,[10077,143...|\n",
      "|       0.0|    0|(65536,[17625,409...|\n",
      "|       0.0|    0|(65536,[2731,6589...|\n",
      "|       1.0|    0|(65536,[12001,382...|\n",
      "|       0.0|    0|(65536,[11985,163...|\n",
      "|       1.0|    0|(65536,[1198,9818...|\n",
      "|       0.0|    0|(65536,[12215,259...|\n",
      "|       1.0|    0|(65536,[17047,273...|\n",
      "|       0.0|    0|(65536,[8227,3860...|\n",
      "|       1.0|    0|(65536,[368,12350...|\n",
      "|       0.0|    0|(65536,[10796,140...|\n",
      "|       0.0|    0|(65536,[5381,1585...|\n",
      "|       0.0|    0|(65536,[1903,1079...|\n",
      "|       0.0|    0|(65536,[1198,1221...|\n",
      "|       0.0|    0|(65536,[1696,8449...|\n",
      "|       0.0|    0|(65536,[637,5636,...|\n",
      "|       1.0|    0|(65536,[23700,258...|\n",
      "|       1.0|    0|(65536,[11430,116...|\n",
      "|       0.0|    0|(65536,[8317,1789...|\n",
      "|       0.0|    0|(65536,[19121,191...|\n",
      "|       0.0|    0|(65536,[21823,328...|\n",
      "|       0.0|    0|(65536,[9496,1101...|\n",
      "|       0.0|    0|(65536,[2284,2464...|\n",
      "|       1.0|    0|(65536,[2762,4192...|\n",
      "|       1.0|    0|(65536,[5381,1291...|\n",
      "|       0.0|    0|(65536,[2973,7212...|\n",
      "|       1.0|    0|(65536,[6589,2958...|\n",
      "|       0.0|    0|(65536,[3191,3311...|\n",
      "|       1.0|    0|(65536,[1198,1589...|\n",
      "|       0.0|    0|(65536,[12,521,31...|\n",
      "|       0.0|    0|(65536,[6643,3288...|\n",
      "|       0.0|    0|(65536,[27308,642...|\n",
      "|       1.0|    0|(65536,[1094,9713...|\n",
      "|       0.0|    0|(65536,[9084,1762...|\n",
      "|       0.0|    0|(65536,[14243,156...|\n",
      "|       1.0|    0|(65536,[1870,1164...|\n",
      "|       0.0|    0|(65536,[47050,552...|\n",
      "|       0.0|    0|(65536,[3329,1915...|\n",
      "|       1.0|    0|(65536,[7301,3235...|\n",
      "|       0.0|    0|(65536,[3976,1271...|\n",
      "|       0.0|    0|(65536,[7823,1056...|\n",
      "|       0.0|    0|(65536,[5816,6042...|\n",
      "|       0.0|    0|(65536,[14827,227...|\n",
      "|       0.0|    0|(65536,[1602,4143...|\n",
      "|       1.0|    0|(65536,[6397,7625...|\n",
      "|       0.0|    0|(65536,[1927,5877...|\n",
      "|       0.0|    0|(65536,[15531,260...|\n",
      "|       0.0|    0|(65536,[10042,191...|\n",
      "|       0.0|    0|(65536,[6413,8538...|\n",
      "|       1.0|    0|(65536,[1602,9859...|\n",
      "|       1.0|    0|(65536,[6498,6693...|\n",
      "|       1.0|    0|(65536,[2171,1804...|\n",
      "|       0.0|    0|(65536,[14676,618...|\n",
      "|       0.0|    0|(65536,[21737,449...|\n",
      "|       0.0|    0|(65536,[5660,4813...|\n",
      "|       0.0|    0|(65536,[5266,1127...|\n",
      "|       0.0|    0|(65536,[518,1198,...|\n",
      "|       0.0|    0|(65536,[1303,1613...|\n",
      "|       0.0|    0|(65536,[57810],[6...|\n",
      "|       0.0|    0|(65536,[11228,164...|\n",
      "|       1.0|    0|(65536,[24297,386...|\n",
      "|       0.0|    0|(65536,[2606,5964...|\n",
      "|       0.0|    0|(65536,[6040,1199...|\n",
      "|       0.0|    0|(65536,[11650,271...|\n",
      "|       0.0|    0|(65536,[7663,1359...|\n",
      "|       0.0|    0|(65536,[9129,1221...|\n",
      "|       0.0|    0|(65536,[6136,4029...|\n",
      "|       0.0|    0|(65536,[7173,2881...|\n",
      "|       0.0|    0|(65536,[465,4832,...|\n",
      "|       0.0|    0|(65536,[4271,6661...|\n",
      "|       0.0|    0|(65536,[4832,7491...|\n",
      "|       0.0|    0|(65536,[17152,259...|\n",
      "|       0.0|    0|(65536,[1821,6946...|\n",
      "|       1.0|    0|(65536,[15507,232...|\n",
      "|       0.0|    0|(65536,[23994,361...|\n",
      "|       1.0|    0|(65536,[17625,181...|\n",
      "|       0.0|    0|(65536,[12675,176...|\n",
      "|       0.0|    0|(65536,[17625,343...|\n",
      "|       0.0|    0|(65536,[16967,176...|\n",
      "|       0.0|    0|(65536,[178,2600,...|\n",
      "|       0.0|    0|(65536,[5381,7529...|\n",
      "|       0.0|    0|(65536,[1628,2235...|\n",
      "|       0.0|    0|(65536,[518,7173,...|\n",
      "|       0.0|    0|(65536,[3734,2270...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "nb_predictions.select(\"prediction\", \"label\", \"features\").show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1781118c",
   "metadata": {
    "id": "1781118c",
    "outputId": "2abae7e7-7332-42f1-8c20-8e0d9b8f182f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/16 19:40:34 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "[Stage 120:===========>                                             (1 + 4) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of NaiveBayes is = 0.373336\n",
      "Test Error of NaiveBayes = 0.626664 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 120:==================================>                      (3 + 2) / 5]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#evaluate and print the accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "nb_accuracy = evaluator.evaluate(nb_predictions)\n",
    "print(\"Accuracy of NaiveBayes is = %g\"% (nb_accuracy))\n",
    "print(\"Test Error of NaiveBayes = %g \" % (1.0 - nb_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "56a4cd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab1_NLP-Sentiments_Analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
