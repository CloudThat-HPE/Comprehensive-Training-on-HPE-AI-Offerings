{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e99a8bd",
   "metadata": {},
   "source": [
    "# Fundamentals of SmartSim and Online Inferencing of Machine Learning Model\n",
    "\n",
    "In this lab we are learning the fundamental concepts in SmartSim and online inferencing of machine learning model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7079bfa4",
   "metadata": {},
   "source": [
    "We are importing Experiment library from SmartSIm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae7d2382",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smartsim import Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7622aa",
   "metadata": {},
   "source": [
    "# Experiment\n",
    "The Experiment acts as both a factory class for constructing the stages of an experiment (Model, Ensemble, Orchestrator, etc.) as well as an interface to interact with the entities created by the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe9b0930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init Experiment and specify to launch locally\n",
    "exp = Experiment(name=\"first-experiment\", launcher=\"local\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae68967c",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "Models are subclasses of SmartSimEntities and are created through the Experiment API. Models represent any computational kernel. Models are flexible enough to support many different applications, however, to be used with our clients (SmartRedis) the application will have to be written in Python, C, C++, or Fortran.\n",
    "\n",
    "\n",
    "## Run settings\n",
    "Models are given RunSettings objects that specify how a kernel should be executed with regard to the workload manager (e.g. Slurm) and the available compute resources on the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2de31978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings to execute the command \"echo hello!\"\n",
    "settings = exp.create_run_settings(exe=\"echo\", exe_args=\"hello!\", run_command=None)\n",
    "\n",
    "# create the simple model instance so we can run it.\n",
    "M1 = exp.create_model(name=\"tutorial-model\", run_settings=settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c179e5",
   "metadata": {},
   "source": [
    "starting model on our experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9768ed1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05:57:17 vm1 SmartSim[2492] INFO \n",
      "\n",
      "=== Launch Summary ===\n",
      "Experiment: first-experiment\n",
      "Experiment Path: /home/azureuser/first-experiment\n",
      "Launcher: local\n",
      "Models: 1\n",
      "Database Status: inactive\n",
      "\n",
      "=== Models ===\n",
      "tutorial-model\n",
      "Executable: /usr/bin/echo\n",
      "Executable Arguments: hello!\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05:57:29 vm1 SmartSim[2492] INFO tutorial-model(2698): Completed\n"
     ]
    }
   ],
   "source": [
    "exp.start(M1, block=True, summary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76075641",
   "metadata": {},
   "source": [
    "Reading the output and error data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ce3d8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content of tutorial-model.out:\n",
      "hello!\n",
      "\n",
      "Content of tutorial-model.err:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outputfile = './tutorial-model.out'\n",
    "errorfile = './tutorial-model.err'\n",
    "\n",
    "print(\"Content of tutorial-model.out:\")\n",
    "with open(outputfile, 'r') as fin:\n",
    "    print(fin.read())\n",
    "print(\"Content of tutorial-model.err:\")\n",
    "with open(errorfile, 'r') as fin:\n",
    "    print(fin.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68538516",
   "metadata": {},
   "source": [
    "Running two concurrent models on an experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce347d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05:57:43 vm1 SmartSim[2492] INFO tutorial-model-1(2703): Completed\n",
      "05:57:46 vm1 SmartSim[2492] INFO tutorial-model-2(2704): Running\n",
      "05:57:47 vm1 SmartSim[2492] INFO tutorial-model-2(2704): Completed\n"
     ]
    }
   ],
   "source": [
    "run_settings_1 = exp.create_run_settings(exe=\"echo\", exe_args=\"hello!\", run_command=None)\n",
    "run_settings_2 = exp.create_run_settings(exe=\"sleep\", exe_args=\"5\", run_command=None)\n",
    "model_1 = exp.create_model(\"tutorial-model-1\", run_settings_1)\n",
    "model_2 = exp.create_model(\"tutorial-model-2\", run_settings_2)\n",
    "exp.start(model_1, model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b903da65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content of tutorial-model-1.out:\n",
      "hello!\n",
      "\n",
      "Content of tutorial-model-2.out:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outputfilenew = './tutorial-model-1.out'\n",
    "outputfilenew1 = './tutorial-model-2.out'\n",
    "\n",
    "print(\"Content of tutorial-model-1.out:\")\n",
    "with open(outputfilenew, 'r') as fin:\n",
    "    print(fin.read())\n",
    "print(\"Content of tutorial-model-2.out:\")\n",
    "with open(outputfilenew1, 'r') as fin:\n",
    "    print(fin.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20200417",
   "metadata": {},
   "source": [
    "# Ensembles\n",
    "\n",
    "SmartSim has the ability to launch an Ensemble of Model applications simultaneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d00643c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define how we want each ensemble member to execute\n",
    "# in this case we create settings to execute \"sleep 3\"\n",
    "ens_settings = exp.create_run_settings(exe=\"sleep\", exe_args=\"3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50ccbaa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05:58:03 vm1 SmartSim[2492] INFO \n",
      "\n",
      "=== Launch Summary ===\n",
      "Experiment: first-experiment\n",
      "Experiment Path: /home/azureuser/first-experiment\n",
      "Launcher: local\n",
      "Ensembles: 1\n",
      "Database Status: inactive\n",
      "\n",
      "=== Ensembles ===\n",
      "ensemble-replica\n",
      "Members: 4\n",
      "Batch Launch: False\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05:58:18 vm1 SmartSim[2492] INFO ensemble-replica_0(2710): Completed\n",
      "05:58:18 vm1 SmartSim[2492] INFO ensemble-replica_2(2712): Completed\n",
      "05:58:19 vm1 SmartSim[2492] INFO ensemble-replica_1(2711): Completed\n",
      "05:58:19 vm1 SmartSim[2492] INFO ensemble-replica_3(2713): Completed\n",
      "05:58:20 vm1 SmartSim[2492] INFO ensemble-replica_1(2711): Completed\n",
      "05:58:20 vm1 SmartSim[2492] INFO ensemble-replica_3(2713): Completed\n"
     ]
    }
   ],
   "source": [
    "ensemble = exp.create_ensemble(\"ensemble-replica\",\n",
    "                               replicas=4,\n",
    "                               run_settings=ens_settings)\n",
    "\n",
    "exp.start(ensemble, summary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24a83599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05:58:38 vm1 SmartSim[2492] INFO \n",
      "\n",
      "=== Launch Summary ===\n",
      "Experiment: first-experiment\n",
      "Experiment Path: /home/azureuser/first-experiment\n",
      "Launcher: local\n",
      "Ensembles: 1\n",
      "Database Status: inactive\n",
      "\n",
      "=== Ensembles ===\n",
      "ensemble-replica\n",
      "Members: 4\n",
      "Batch Launch: False\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05:58:51 vm1 SmartSim[2492] INFO ensemble-replica_0(2719): Completed\n",
      "05:58:51 vm1 SmartSim[2492] INFO ensemble-replica_1(2720): Completed\n",
      "05:58:51 vm1 SmartSim[2492] INFO ensemble-replica_2(2721): Completed\n",
      "05:58:53 vm1 SmartSim[2492] INFO ensemble-replica_3(2722): Completed\n"
     ]
    }
   ],
   "source": [
    "ens_settings1 = exp.create_run_settings(exe=\"echo\", exe_args=\"hello-world\")\n",
    "\n",
    "\n",
    "ensemble1 = exp.create_ensemble(\"ensemble-replica\",\n",
    "                               replicas=4,\n",
    "                               run_settings=ens_settings1)\n",
    "\n",
    "exp.start(ensemble1, summary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad9a199",
   "metadata": {},
   "source": [
    "pip install smartredis[dev]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787a5ff6",
   "metadata": {},
   "source": [
    "# Orchestrator\n",
    "\n",
    "The Orchestrator is an in-memory database that is launched prior to all other entities within an Experiment. The Orchestrator can be used to store and retrieve data during the course of an experiment and across multiple entities. In order to stream data into or receive data from the Orchestrator, one of the SmartSim clients (SmartRedis) has to be used within a Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96ffd5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smartredis import Client\n",
    "import numpy as np\n",
    "\n",
    "REDIS_PORT=6899"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "247b218b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05:59:12 vm1 SmartSim[2492] INFO Working in previously created experiment\n"
     ]
    }
   ],
   "source": [
    "# start a new Experiment for this section\n",
    "exp = Experiment(\"tutorial-smartredis\", launcher=\"local\")\n",
    "\n",
    "# create and start an instance of the Orchestrator database\n",
    "db = exp.create_database(db_nodes=1,\n",
    "                         port=REDIS_PORT,\n",
    "                         interface=\"lo\")\n",
    "# create an output directory for the database log files\n",
    "exp.generate(db)\n",
    "\n",
    "# start the database\n",
    "exp.start(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16624fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect a SmartRedis client at the address supplied by the launched\n",
    "# Orchestrator instance.\n",
    "# Cluster=False as the Orchestrator was deployed on a single compute host (local)\n",
    "client = Client(address=db.get_address()[0], cluster=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35e8ee77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receive tensor:\n",
      "\n",
      " [[[1. 1. 1.]\n",
      "  [1. 1. 1.]\n",
      "  [1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1.]\n",
      "  [1. 1. 1.]\n",
      "  [1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1.]\n",
      "  [1. 1. 1.]\n",
      "  [1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1.]\n",
      "  [1. 1. 1.]\n",
      "  [1. 1. 1.]]]\n"
     ]
    }
   ],
   "source": [
    "send_tensor = np.ones((4,3,3))\n",
    "\n",
    "client.put_tensor(\"tutorial_tensor_1\", send_tensor)\n",
    "\n",
    "receive_tensor = client.get_tensor(\"tutorial_tensor_1\")\n",
    "\n",
    "print('Receive tensor:\\n\\n', receive_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "91158b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05:59:55 vm1 SmartSim[2492] INFO Stopping model orchestrator_0 with job name orchestrator_0-CNELZGS4HLQT\n"
     ]
    }
   ],
   "source": [
    "exp.stop(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89613d18",
   "metadata": {},
   "source": [
    "# Inference ML model using SmartSim\n",
    "\n",
    "Combined with the SmartRedis clients, the Orchestrator is capable of hosting and executing AI models written in Python on CPU or GPU. The Orchestrator supports models written with TensorFlow, Pytorch, TensorFlow-Lite, or models saved in an ONNX format (e.g. sci-kit learn). Here we are using TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5da6d2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some helper libraries for the tutorial\n",
    "import io\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "# import smartsim and smartredis\n",
    "from smartredis import Client\n",
    "from smartsim import Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a77581a",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = Experiment(\"Inference-Tutorial\", launcher=\"local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58a4a656",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = exp.create_database(port=6780, interface=\"lo\")\n",
    "exp.start(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5371c45c",
   "metadata": {},
   "source": [
    "Now we can create a very simple fully connected network on TensorFlow.Keras API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aee25603",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "# create a simple Fully connected network in Keras\n",
    "model = keras.Sequential(\n",
    "    layers=[\n",
    "        keras.layers.InputLayer(input_shape=(28, 28), name=\"input\"),\n",
    "        keras.layers.Flatten(input_shape=(28, 28), name=\"flatten\"),\n",
    "        keras.layers.Dense(128, activation=\"relu\", name=\"dense\"),\n",
    "        keras.layers.Dense(10, activation=\"softmax\", name=\"output\"),\n",
    "    ],\n",
    "    name=\"FCN\",\n",
    ")\n",
    "\n",
    "# Compile model with optimizer\n",
    "model.compile(optimizer=\"adam\",\n",
    "            loss=\"sparse_categorical_crossentropy\",\n",
    "            metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea313bd",
   "metadata": {},
   "source": [
    "After a model is created (trained or not), the graph of the model is frozen and saved to file so the client method client.set_model_from_file can load it into the database.\n",
    "\n",
    "SmartSim includes a utility to freeze the graph of a TensorFlow or Keras model in smartsim.ml.tf. To use TensorFlow or Keras in SmartSim, specify TF as the argument for backend in the call to client.set_model or client.set_model_from_file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9339fd70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.13783866 0.08369803 0.10155473 0.08805773 0.0864891  0.16028616\n",
      "  0.11356978 0.08432745 0.03862334 0.10555508]]\n"
     ]
    }
   ],
   "source": [
    "client = Client(address=db.get_address()[0], cluster=False)\n",
    "\n",
    "from smartsim.ml.tf import freeze_model\n",
    "\n",
    "# SmartSim utility for Freezing the model and saving it to a file.\n",
    "model_path, inputs, outputs = freeze_model(model, os.getcwd(), \"fcn.pb\")\n",
    "\n",
    "# use the same client we used for PyTorch to set the TensorFlow model\n",
    "# this time the method for setting a model from a saved file is shown.\n",
    "# TensorFlow backed requires named inputs and outputs on graph\n",
    "# this differs from PyTorch and ONNX.\n",
    "client.set_model_from_file(\n",
    "    \"keras_fcn\", model_path, \"TF\", device=\"CPU\", inputs=inputs, outputs=outputs\n",
    ")\n",
    "\n",
    "# put random random input tensor into the database\n",
    "input_data = np.random.rand(1, 28, 28).astype(np.float32)\n",
    "client.put_tensor(\"input\", input_data)\n",
    "\n",
    "# run the Fully Connected Network model on the tensor we just put\n",
    "# in and store the result of the inference at the \"output\" key\n",
    "client.run_model(\"keras_fcn\", \"input\", \"output\")\n",
    "\n",
    "# get the result of the inference\n",
    "pred = client.get_tensor(\"output\")\n",
    "print(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
